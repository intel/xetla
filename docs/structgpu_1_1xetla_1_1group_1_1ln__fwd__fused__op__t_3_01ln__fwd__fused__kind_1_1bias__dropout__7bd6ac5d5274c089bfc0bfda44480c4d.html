<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>XeTLA: gpu::xetla::group::ln_fwd_fused_op_t&lt; ln_fwd_fused_kind::bias_dropout_resAdd_ln, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, gpu_arch::Xe &gt; Struct Template Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">XeTLA<span id="projectnumber">&#160;v0.3.3</span>
   </div>
   <div id="projectbrief">IntelÂ® Xe Templates for Linear Algebra - API Definition Document</div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-types">Public Types</a> &#124;
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="#pub-static-attribs">Static Public Attributes</a> &#124;
<a href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__58b7267af4cd70354b811bcd0b820443.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">gpu::xetla::group::ln_fwd_fused_op_t&lt; ln_fwd_fused_kind::bias_dropout_resAdd_ln, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, gpu_arch::Xe &gt; Struct Template Reference</div></div>
</div><!--header-->
<div class="contents">

<p><code>#include &lt;<a class="el" href="layer__norm__fused__op__fwd__xe_8hpp_source.html">layer_norm_fused_op_fwd_xe.hpp</a>&gt;</code></p>
<div class="dynheader">
Collaboration diagram for gpu::xetla::group::ln_fwd_fused_op_t&lt; ln_fwd_fused_kind::bias_dropout_resAdd_ln, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, gpu_arch::Xe &gt;:</div>
<div class="dyncontent">
<div class="center"><img src="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__e0061928d04099355964695c2c11cbfb.png" border="0" usemap="#agpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__resAdd__ln_00_01dtype__in___00_01dtype__out___00_01dtype__acc___00_01layer__norm__attr___00_01gpu__arch_1_1Xe_01_4_coll__map" alt="Collaboration graph"/></div>
<map name="agpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__resAdd__ln_00_01dtype__in___00_01dtype__out___00_01dtype__acc___00_01layer__norm__attr___00_01gpu__arch_1_1Xe_01_4_coll__map" id="agpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__resAdd__ln_00_01dtype__in___00_01dtype__out___00_01dtype__acc___00_01layer__norm__attr___00_01gpu__arch_1_1Xe_01_4_coll__map">
<area shape="rect" title=" " alt="" coords="571,213,800,311"/>
<area shape="rect" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html" title=" " alt="" coords="218,5,358,60"/>
<area shape="rect" title=" " alt="" coords="5,99,133,124"/>
<area shape="rect" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html" title=" " alt="" coords="216,84,360,139"/>
<area shape="rect" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html" title=" " alt="" coords="215,163,361,217"/>
<area shape="rect" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html" title=" " alt="" coords="183,242,393,341"/>
<area shape="rect" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html" title=" " alt="" coords="187,365,389,463"/>
<area shape="rect" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html" title=" " alt="" coords="187,487,389,586"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-types" name="pub-types"></a>
Public Types</h2></td></tr>
<tr class="memitem:a8374ffb1cd05109a508506433904c0df"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8374ffb1cd05109a508506433904c0df">dtype_acc</a> = dtype_acc_</td></tr>
<tr class="separator:a8374ffb1cd05109a508506433904c0df"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8effd0e1b0a2021bdb919840e3e6fbe6"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8effd0e1b0a2021bdb919840e3e6fbe6">dtype_in</a> = dtype_in_</td></tr>
<tr class="separator:a8effd0e1b0a2021bdb919840e3e6fbe6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab548c9dc0c1a8077991feb1cd43142ec"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#ab548c9dc0c1a8077991feb1cd43142ec">dtype_out</a> = dtype_out_</td></tr>
<tr class="separator:ab548c9dc0c1a8077991feb1cd43142ec"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8be8bd2a2d751e04195df7c9ba4d940d"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8be8bd2a2d751e04195df7c9ba4d940d">dtype_mask</a> = uint8_t</td></tr>
<tr class="separator:a8be8bd2a2d751e04195df7c9ba4d940d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae459151ba321c0996a27646b365744d4"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#ae459151ba321c0996a27646b365744d4">arguments_t</a> = <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__arguments__t.html">ln_fwd_fused_op_arguments_t</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8effd0e1b0a2021bdb919840e3e6fbe6">dtype_in</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#ab548c9dc0c1a8077991feb1cd43142ec">dtype_out</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8374ffb1cd05109a508506433904c0df">dtype_acc</a> &gt;</td></tr>
<tr class="separator:ae459151ba321c0996a27646b365744d4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0167ac6b68ca9e4b476dc733c97853e1"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a> = <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1tile__desc__t.html">subgroup::tile_desc_t</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#abbad839064868aeea3ac306b17756ccd">chunk_size</a>, 1, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#abbad839064868aeea3ac306b17756ccd">chunk_size</a>, 1, <a class="el" href="namespacegpu_1_1xetla.html#a51137fd81d0d9d2156525a1e279432aaa78506882d645395a052df8b01a927395">reg_layout::tiled</a> &gt;</td></tr>
<tr class="separator:a0167ac6b68ca9e4b476dc733c97853e1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9f71fb5bb23b52eeae10a27610be999f"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a9f71fb5bb23b52eeae10a27610be999f">bias_in_t</a> = <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">subgroup::tile_t</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8effd0e1b0a2021bdb919840e3e6fbe6">dtype_in</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a> &gt;</td></tr>
<tr class="separator:a9f71fb5bb23b52eeae10a27610be999f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a80e449100ea335a7771f182b692a6e30"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a80e449100ea335a7771f182b692a6e30">bias_in_payload_t</a> = <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">subgroup::mem_payload_t</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8effd0e1b0a2021bdb919840e3e6fbe6">dtype_in</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a>, <a class="el" href="namespacegpu_1_1xetla.html#aa8afe1d12e7777419fb6ea09534a0aa7a5baf9a1aba8e4b6fa44c20aafacc4233">msg_type::block_1d</a>, <a class="el" href="namespacegpu_1_1xetla.html#af4a355a1806510c5515fad16f5910561a641fabb8e5e7d1d0333e2c9c384f959c">mem_layout::row_major</a>, <a class="el" href="namespacegpu_1_1xetla.html#a7f225ed816e841c1d31414d872dae59da9c70933aff6b2a6d08c687a6cbb6b765">mem_space::global</a>, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;</td></tr>
<tr class="separator:a80e449100ea335a7771f182b692a6e30"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4fc418cfed7e7c2139ba5a9c0c34e9a7"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a4fc418cfed7e7c2139ba5a9c0c34e9a7">res_in_t</a> = <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">subgroup::tile_t</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8effd0e1b0a2021bdb919840e3e6fbe6">dtype_in</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a> &gt;</td></tr>
<tr class="separator:a4fc418cfed7e7c2139ba5a9c0c34e9a7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7dc1d123523470abb2cf7b9219790bc6"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a7dc1d123523470abb2cf7b9219790bc6">res_in_payload_t</a> = <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">subgroup::mem_payload_t</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8effd0e1b0a2021bdb919840e3e6fbe6">dtype_in</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a>, <a class="el" href="namespacegpu_1_1xetla.html#aa8afe1d12e7777419fb6ea09534a0aa7a5baf9a1aba8e4b6fa44c20aafacc4233">msg_type::block_1d</a>, <a class="el" href="namespacegpu_1_1xetla.html#af4a355a1806510c5515fad16f5910561a641fabb8e5e7d1d0333e2c9c384f959c">mem_layout::row_major</a>, <a class="el" href="namespacegpu_1_1xetla.html#a7f225ed816e841c1d31414d872dae59da9c70933aff6b2a6d08c687a6cbb6b765">mem_space::global</a>, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;</td></tr>
<tr class="separator:a7dc1d123523470abb2cf7b9219790bc6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2d7ff9f0cc50829c059ee7736fab7a8e"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a2d7ff9f0cc50829c059ee7736fab7a8e">mask_in_t</a> = <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">subgroup::tile_t</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8be8bd2a2d751e04195df7c9ba4d940d">dtype_mask</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a> &gt;</td></tr>
<tr class="separator:a2d7ff9f0cc50829c059ee7736fab7a8e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a531394b46ed357f0feb05eb8800ca5f9"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a531394b46ed357f0feb05eb8800ca5f9">mask_in_payload_t</a> = <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">subgroup::mem_payload_t</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8be8bd2a2d751e04195df7c9ba4d940d">dtype_mask</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a>, <a class="el" href="namespacegpu_1_1xetla.html#aa8afe1d12e7777419fb6ea09534a0aa7a5baf9a1aba8e4b6fa44c20aafacc4233">msg_type::block_1d</a>, <a class="el" href="namespacegpu_1_1xetla.html#af4a355a1806510c5515fad16f5910561a641fabb8e5e7d1d0333e2c9c384f959c">mem_layout::row_major</a>, <a class="el" href="namespacegpu_1_1xetla.html#a7f225ed816e841c1d31414d872dae59da9c70933aff6b2a6d08c687a6cbb6b765">mem_space::global</a>, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;</td></tr>
<tr class="separator:a531394b46ed357f0feb05eb8800ca5f9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a50d4e962439f2a2f7bb9568c7bf68d11"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a50d4e962439f2a2f7bb9568c7bf68d11">bias_dropout_res_out_t</a> = <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">subgroup::tile_t</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#ab548c9dc0c1a8077991feb1cd43142ec">dtype_out</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a> &gt;</td></tr>
<tr class="separator:a50d4e962439f2a2f7bb9568c7bf68d11"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9d0578e62f9e29c083f6eeb2bf72f9d9"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a9d0578e62f9e29c083f6eeb2bf72f9d9">bias_dropout_res_out_payload_t</a> = <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">subgroup::mem_payload_t</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#ab548c9dc0c1a8077991feb1cd43142ec">dtype_out</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a>, <a class="el" href="namespacegpu_1_1xetla.html#aa8afe1d12e7777419fb6ea09534a0aa7a5baf9a1aba8e4b6fa44c20aafacc4233">msg_type::block_1d</a>, <a class="el" href="namespacegpu_1_1xetla.html#af4a355a1806510c5515fad16f5910561a641fabb8e5e7d1d0333e2c9c384f959c">mem_layout::row_major</a>, <a class="el" href="namespacegpu_1_1xetla.html#a7f225ed816e841c1d31414d872dae59da9c70933aff6b2a6d08c687a6cbb6b765">mem_space::global</a>, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;</td></tr>
<tr class="separator:a9d0578e62f9e29c083f6eeb2bf72f9d9"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:ac074f8cc33d843ff2769e00be0b1dca5"><td class="memItemLeft" align="right" valign="top"><a class="el" href="common_2core_2common_8hpp.html#a9ed53999886ec13b86a4fe2e0fc16765">__XETLA_API</a> void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#ac074f8cc33d843ff2769e00be0b1dca5">init</a> (<a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#ae459151ba321c0996a27646b365744d4">arguments_t</a> *args, uint32_t wg_idx, uint32_t wg_idy, uint32_t sg_idx, uint32_t sg_idy, uint32_t start_m)</td></tr>
<tr class="separator:ac074f8cc33d843ff2769e00be0b1dca5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af7cf66d03999e43e03066945a2dd8e56"><td class="memItemLeft" align="right" valign="top"><a class="el" href="common_2core_2common_8hpp.html#a9ed53999886ec13b86a4fe2e0fc16765">__XETLA_API</a> <a class="el" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8374ffb1cd05109a508506433904c0df">dtype_acc</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#abbad839064868aeea3ac306b17756ccd">chunk_size</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#af7cf66d03999e43e03066945a2dd8e56">pre_op</a> (<a class="el" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8374ffb1cd05109a508506433904c0df">dtype_acc</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#abbad839064868aeea3ac306b17756ccd">chunk_size</a> &gt; input)</td></tr>
<tr class="separator:af7cf66d03999e43e03066945a2dd8e56"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a451875db45fea17b9a83d2360a1289e5"><td class="memItemLeft" align="right" valign="top"><a class="el" href="common_2core_2common_8hpp.html#a9ed53999886ec13b86a4fe2e0fc16765">__XETLA_API</a> <a class="el" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8374ffb1cd05109a508506433904c0df">dtype_acc</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#abbad839064868aeea3ac306b17756ccd">chunk_size</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a451875db45fea17b9a83d2360a1289e5">post_op</a> (<a class="el" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8374ffb1cd05109a508506433904c0df">dtype_acc</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#abbad839064868aeea3ac306b17756ccd">chunk_size</a> &gt; input)</td></tr>
<tr class="separator:a451875db45fea17b9a83d2360a1289e5"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-attribs" name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr class="memitem:af7df39edf03895c9b5447dd702bfe0ee"><td class="memItemLeft" align="right" valign="top"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a9f71fb5bb23b52eeae10a27610be999f">bias_in_t</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#af7df39edf03895c9b5447dd702bfe0ee">bias_in</a></td></tr>
<tr class="separator:af7df39edf03895c9b5447dd702bfe0ee"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8723833ccf23122c470f0919bf2105bd"><td class="memItemLeft" align="right" valign="top"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a80e449100ea335a7771f182b692a6e30">bias_in_payload_t</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8723833ccf23122c470f0919bf2105bd">bias_in_payload</a></td></tr>
<tr class="separator:a8723833ccf23122c470f0919bf2105bd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab290fcd351f7b2d194c2bb93e0d10dd5"><td class="memItemLeft" align="right" valign="top"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a50d4e962439f2a2f7bb9568c7bf68d11">bias_dropout_res_out_t</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#ab290fcd351f7b2d194c2bb93e0d10dd5">bias_dropout_res_out</a></td></tr>
<tr class="separator:ab290fcd351f7b2d194c2bb93e0d10dd5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a324d69157219c9f25037cf688efdf3c8"><td class="memItemLeft" align="right" valign="top"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a9d0578e62f9e29c083f6eeb2bf72f9d9">bias_dropout_res_out_payload_t</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a324d69157219c9f25037cf688efdf3c8">bias_dropout_res_out_payload</a></td></tr>
<tr class="separator:a324d69157219c9f25037cf688efdf3c8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a308f91070f0b7badd144655e71150506"><td class="memItemLeft" align="right" valign="top"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a4fc418cfed7e7c2139ba5a9c0c34e9a7">res_in_t</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a308f91070f0b7badd144655e71150506">res_in</a></td></tr>
<tr class="separator:a308f91070f0b7badd144655e71150506"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a70e0ba488891742486a946d9a96572b3"><td class="memItemLeft" align="right" valign="top"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a7dc1d123523470abb2cf7b9219790bc6">res_in_payload_t</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a70e0ba488891742486a946d9a96572b3">res_in_payload</a></td></tr>
<tr class="separator:a70e0ba488891742486a946d9a96572b3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6d497418df492c5a4a98c772001522e6"><td class="memItemLeft" align="right" valign="top"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a2d7ff9f0cc50829c059ee7736fab7a8e">mask_in_t</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a6d497418df492c5a4a98c772001522e6">mask_in</a></td></tr>
<tr class="separator:a6d497418df492c5a4a98c772001522e6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a115d5ce3e8d492d4aa2c624432c2dc6d"><td class="memItemLeft" align="right" valign="top"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a531394b46ed357f0feb05eb8800ca5f9">mask_in_payload_t</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a115d5ce3e8d492d4aa2c624432c2dc6d">mask_in_payload</a></td></tr>
<tr class="separator:a115d5ce3e8d492d4aa2c624432c2dc6d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a74cf23973a27bac115cc6e64af12d061"><td class="memItemLeft" align="right" valign="top">uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a74cf23973a27bac115cc6e64af12d061">mat_ld</a></td></tr>
<tr class="separator:a74cf23973a27bac115cc6e64af12d061"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a584f628ad8e0adb2f1e5723539e4ce56"><td class="memItemLeft" align="right" valign="top">uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a584f628ad8e0adb2f1e5723539e4ce56">mask_ld</a></td></tr>
<tr class="separator:a584f628ad8e0adb2f1e5723539e4ce56"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2730dc5052431571c642dc08245d758d"><td class="memItemLeft" align="right" valign="top">uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a2730dc5052431571c642dc08245d758d">matrix_n</a></td></tr>
<tr class="separator:a2730dc5052431571c642dc08245d758d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8801d22bdbed500c4d5c15af0cbefd6c"><td class="memItemLeft" align="right" valign="top">uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8801d22bdbed500c4d5c15af0cbefd6c">matrix_m</a></td></tr>
<tr class="separator:a8801d22bdbed500c4d5c15af0cbefd6c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a64b47183bccbc1882750636d92a8bcf6"><td class="memItemLeft" align="right" valign="top">float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a64b47183bccbc1882750636d92a8bcf6">dropout_scale</a></td></tr>
<tr class="separator:a64b47183bccbc1882750636d92a8bcf6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a041ae1afaf0a12ba2a800120ad2fa6e5"><td class="memItemLeft" align="right" valign="top">float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a041ae1afaf0a12ba2a800120ad2fa6e5">dropout_prob</a></td></tr>
<tr class="separator:a041ae1afaf0a12ba2a800120ad2fa6e5"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-static-attribs" name="pub-static-attribs"></a>
Static Public Attributes</h2></td></tr>
<tr class="memitem:a2fbc396287e5c4333e17d5691bfa60d1"><td class="memItemLeft" align="right" valign="top">static constexpr <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7ec">ln_fwd_fused_kind</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a2fbc396287e5c4333e17d5691bfa60d1">fused_op_kind</a> = <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a></td></tr>
<tr class="separator:a2fbc396287e5c4333e17d5691bfa60d1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2c4b1f85f8d84ba460329ada6f660450"><td class="memItemLeft" align="right" valign="top">static constexpr uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a2c4b1f85f8d84ba460329ada6f660450">wg_tile_m</a> = layer_norm_attr_::wg_tile_m</td></tr>
<tr class="separator:a2c4b1f85f8d84ba460329ada6f660450"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa6da3ba31fa9091675265e2157ce9437"><td class="memItemLeft" align="right" valign="top">static constexpr uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#aa6da3ba31fa9091675265e2157ce9437">wg_tile_n</a> = layer_norm_attr_::wg_tile_n</td></tr>
<tr class="separator:aa6da3ba31fa9091675265e2157ce9437"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a756569efe5da843eb01bcd9dc7ea4573"><td class="memItemLeft" align="right" valign="top">static constexpr uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a756569efe5da843eb01bcd9dc7ea4573">sg_tile_m</a> = layer_norm_attr_::sg_tile_m</td></tr>
<tr class="separator:a756569efe5da843eb01bcd9dc7ea4573"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a686ec6a547e37f87d42a046aaabbef8b"><td class="memItemLeft" align="right" valign="top">static constexpr uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a686ec6a547e37f87d42a046aaabbef8b">sg_tile_n</a> = layer_norm_attr_::sg_tile_n</td></tr>
<tr class="separator:a686ec6a547e37f87d42a046aaabbef8b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5c72cd21e83ad0999a8b5e684748c414"><td class="memItemLeft" align="right" valign="top">static constexpr uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a5c72cd21e83ad0999a8b5e684748c414">wg_num_m</a> = layer_norm_attr_::wg_num_m</td></tr>
<tr class="separator:a5c72cd21e83ad0999a8b5e684748c414"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a49110b26a2b2afe3b1e2ea65d2cf5187"><td class="memItemLeft" align="right" valign="top">static constexpr uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a49110b26a2b2afe3b1e2ea65d2cf5187">wg_num_n</a> = layer_norm_attr_::wg_num_n</td></tr>
<tr class="separator:a49110b26a2b2afe3b1e2ea65d2cf5187"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abbad839064868aeea3ac306b17756ccd"><td class="memItemLeft" align="right" valign="top">static constexpr uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#abbad839064868aeea3ac306b17756ccd">chunk_size</a> = layer_norm_attr_::chunk_size</td></tr>
<tr class="separator:abbad839064868aeea3ac306b17756ccd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9334902ba86ffc5c7768e459722ac72c"><td class="memItemLeft" align="right" valign="top">static constexpr uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a9334902ba86ffc5c7768e459722ac72c">n_chunks</a> = <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a686ec6a547e37f87d42a046aaabbef8b">sg_tile_n</a> / <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#abbad839064868aeea3ac306b17756ccd">chunk_size</a></td></tr>
<tr class="separator:a9334902ba86ffc5c7768e459722ac72c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a96cb414a45a6c769e7a6c4347e764e36"><td class="memItemLeft" align="right" valign="top">static constexpr uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a96cb414a45a6c769e7a6c4347e764e36">wg_size_x</a> = (<a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#aa6da3ba31fa9091675265e2157ce9437">wg_tile_n</a> + <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a686ec6a547e37f87d42a046aaabbef8b">sg_tile_n</a> - 1) / <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a686ec6a547e37f87d42a046aaabbef8b">sg_tile_n</a></td></tr>
<tr class="separator:a96cb414a45a6c769e7a6c4347e764e36"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a291a57aaf1f649714e9351f309e8cff7"><td class="memItemLeft" align="right" valign="top">static constexpr uint32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a291a57aaf1f649714e9351f309e8cff7">wg_size_y</a> = (<a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a2c4b1f85f8d84ba460329ada6f660450">wg_tile_m</a> + <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a756569efe5da843eb01bcd9dc7ea4573">sg_tile_m</a> - 1) / <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a756569efe5da843eb01bcd9dc7ea4573">sg_tile_m</a></td></tr>
<tr class="separator:a291a57aaf1f649714e9351f309e8cff7"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><div class="compoundTemplParams">template&lt;typename dtype_in_, typename dtype_out_, typename dtype_acc_, typename layer_norm_attr_&gt;<br />
struct gpu::xetla::group::ln_fwd_fused_op_t&lt; ln_fwd_fused_kind::bias_dropout_resAdd_ln, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, gpu_arch::Xe &gt;</div><dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">dtype_in_</td><td></td></tr>
    <tr><td class="paramname">dtype_out_</td><td></td></tr>
    <tr><td class="paramname">dtype_acc_</td><td></td></tr>
    <tr><td class="paramname">layer_norm_attr_</td><td></td></tr>
  </table>
  </dd>
</dl>
</div><h2 class="groupheader">Member Typedef Documentation</h2>
<a id="ae459151ba321c0996a27646b365744d4" name="ae459151ba321c0996a27646b365744d4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae459151ba321c0996a27646b365744d4">&#9670;&#160;</a></span>arguments_t</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::arguments_t =  <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__arguments__t.html">ln_fwd_fused_op_arguments_t</a>&lt;<a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8effd0e1b0a2021bdb919840e3e6fbe6">dtype_in</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#ab548c9dc0c1a8077991feb1cd43142ec">dtype_out</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8374ffb1cd05109a508506433904c0df">dtype_acc</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a9d0578e62f9e29c083f6eeb2bf72f9d9" name="a9d0578e62f9e29c083f6eeb2bf72f9d9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9d0578e62f9e29c083f6eeb2bf72f9d9">&#9670;&#160;</a></span>bias_dropout_res_out_payload_t</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::bias_dropout_res_out_payload_t =  <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">subgroup::mem_payload_t</a>&lt;<a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#ab548c9dc0c1a8077991feb1cd43142ec">dtype_out</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a>, <a class="el" href="namespacegpu_1_1xetla.html#aa8afe1d12e7777419fb6ea09534a0aa7a5baf9a1aba8e4b6fa44c20aafacc4233">msg_type::block_1d</a>, <a class="el" href="namespacegpu_1_1xetla.html#af4a355a1806510c5515fad16f5910561a641fabb8e5e7d1d0333e2c9c384f959c">mem_layout::row_major</a>, <a class="el" href="namespacegpu_1_1xetla.html#a7f225ed816e841c1d31414d872dae59da9c70933aff6b2a6d08c687a6cbb6b765">mem_space::global</a>, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a50d4e962439f2a2f7bb9568c7bf68d11" name="a50d4e962439f2a2f7bb9568c7bf68d11"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a50d4e962439f2a2f7bb9568c7bf68d11">&#9670;&#160;</a></span>bias_dropout_res_out_t</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::bias_dropout_res_out_t =  <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">subgroup::tile_t</a>&lt;<a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#ab548c9dc0c1a8077991feb1cd43142ec">dtype_out</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a80e449100ea335a7771f182b692a6e30" name="a80e449100ea335a7771f182b692a6e30"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a80e449100ea335a7771f182b692a6e30">&#9670;&#160;</a></span>bias_in_payload_t</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::bias_in_payload_t =  <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">subgroup::mem_payload_t</a>&lt;<a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8effd0e1b0a2021bdb919840e3e6fbe6">dtype_in</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a>, <a class="el" href="namespacegpu_1_1xetla.html#aa8afe1d12e7777419fb6ea09534a0aa7a5baf9a1aba8e4b6fa44c20aafacc4233">msg_type::block_1d</a>, <a class="el" href="namespacegpu_1_1xetla.html#af4a355a1806510c5515fad16f5910561a641fabb8e5e7d1d0333e2c9c384f959c">mem_layout::row_major</a>, <a class="el" href="namespacegpu_1_1xetla.html#a7f225ed816e841c1d31414d872dae59da9c70933aff6b2a6d08c687a6cbb6b765">mem_space::global</a>, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a9f71fb5bb23b52eeae10a27610be999f" name="a9f71fb5bb23b52eeae10a27610be999f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9f71fb5bb23b52eeae10a27610be999f">&#9670;&#160;</a></span>bias_in_t</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::bias_in_t =  <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">subgroup::tile_t</a>&lt;<a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8effd0e1b0a2021bdb919840e3e6fbe6">dtype_in</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a8374ffb1cd05109a508506433904c0df" name="a8374ffb1cd05109a508506433904c0df"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8374ffb1cd05109a508506433904c0df">&#9670;&#160;</a></span>dtype_acc</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::dtype_acc =  dtype_acc_</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a8effd0e1b0a2021bdb919840e3e6fbe6" name="a8effd0e1b0a2021bdb919840e3e6fbe6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8effd0e1b0a2021bdb919840e3e6fbe6">&#9670;&#160;</a></span>dtype_in</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::dtype_in =  dtype_in_</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a8be8bd2a2d751e04195df7c9ba4d940d" name="a8be8bd2a2d751e04195df7c9ba4d940d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8be8bd2a2d751e04195df7c9ba4d940d">&#9670;&#160;</a></span>dtype_mask</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::dtype_mask =  uint8_t</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ab548c9dc0c1a8077991feb1cd43142ec" name="ab548c9dc0c1a8077991feb1cd43142ec"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab548c9dc0c1a8077991feb1cd43142ec">&#9670;&#160;</a></span>dtype_out</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::dtype_out =  dtype_out_</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a0167ac6b68ca9e4b476dc733c97853e1" name="a0167ac6b68ca9e4b476dc733c97853e1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0167ac6b68ca9e4b476dc733c97853e1">&#9670;&#160;</a></span>ln_fwd_tile_desc_t</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::ln_fwd_tile_desc_t =  <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1tile__desc__t.html">subgroup::tile_desc_t</a>&lt;<a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#abbad839064868aeea3ac306b17756ccd">chunk_size</a>, 1, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#abbad839064868aeea3ac306b17756ccd">chunk_size</a>, 1, <a class="el" href="namespacegpu_1_1xetla.html#a51137fd81d0d9d2156525a1e279432aaa78506882d645395a052df8b01a927395">reg_layout::tiled</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a531394b46ed357f0feb05eb8800ca5f9" name="a531394b46ed357f0feb05eb8800ca5f9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a531394b46ed357f0feb05eb8800ca5f9">&#9670;&#160;</a></span>mask_in_payload_t</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::mask_in_payload_t =  <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">subgroup::mem_payload_t</a>&lt;<a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8be8bd2a2d751e04195df7c9ba4d940d">dtype_mask</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a>, <a class="el" href="namespacegpu_1_1xetla.html#aa8afe1d12e7777419fb6ea09534a0aa7a5baf9a1aba8e4b6fa44c20aafacc4233">msg_type::block_1d</a>, <a class="el" href="namespacegpu_1_1xetla.html#af4a355a1806510c5515fad16f5910561a641fabb8e5e7d1d0333e2c9c384f959c">mem_layout::row_major</a>, <a class="el" href="namespacegpu_1_1xetla.html#a7f225ed816e841c1d31414d872dae59da9c70933aff6b2a6d08c687a6cbb6b765">mem_space::global</a>, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a2d7ff9f0cc50829c059ee7736fab7a8e" name="a2d7ff9f0cc50829c059ee7736fab7a8e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2d7ff9f0cc50829c059ee7736fab7a8e">&#9670;&#160;</a></span>mask_in_t</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::mask_in_t =  <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">subgroup::tile_t</a>&lt;<a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8be8bd2a2d751e04195df7c9ba4d940d">dtype_mask</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a7dc1d123523470abb2cf7b9219790bc6" name="a7dc1d123523470abb2cf7b9219790bc6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7dc1d123523470abb2cf7b9219790bc6">&#9670;&#160;</a></span>res_in_payload_t</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::res_in_payload_t =  <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">subgroup::mem_payload_t</a>&lt;<a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8effd0e1b0a2021bdb919840e3e6fbe6">dtype_in</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a>, <a class="el" href="namespacegpu_1_1xetla.html#aa8afe1d12e7777419fb6ea09534a0aa7a5baf9a1aba8e4b6fa44c20aafacc4233">msg_type::block_1d</a>, <a class="el" href="namespacegpu_1_1xetla.html#af4a355a1806510c5515fad16f5910561a641fabb8e5e7d1d0333e2c9c384f959c">mem_layout::row_major</a>, <a class="el" href="namespacegpu_1_1xetla.html#a7f225ed816e841c1d31414d872dae59da9c70933aff6b2a6d08c687a6cbb6b765">mem_space::global</a>, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a4fc418cfed7e7c2139ba5a9c0c34e9a7" name="a4fc418cfed7e7c2139ba5a9c0c34e9a7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4fc418cfed7e7c2139ba5a9c0c34e9a7">&#9670;&#160;</a></span>res_in_t</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::res_in_t =  <a class="el" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">subgroup::tile_t</a>&lt;<a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8effd0e1b0a2021bdb919840e3e6fbe6">dtype_in</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a0167ac6b68ca9e4b476dc733c97853e1">ln_fwd_tile_desc_t</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="ac074f8cc33d843ff2769e00be0b1dca5" name="ac074f8cc33d843ff2769e00be0b1dca5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac074f8cc33d843ff2769e00be0b1dca5">&#9670;&#160;</a></span>init()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="common_2core_2common_8hpp.html#a9ed53999886ec13b86a4fe2e0fc16765">__XETLA_API</a> void <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::init </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#ae459151ba321c0996a27646b365744d4">arguments_t</a> *&#160;</td>
          <td class="paramname"><em>args</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>wg_idx</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>wg_idy</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>sg_idx</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>sg_idy</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>start_m</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">args</td><td></td></tr>
    <tr><td class="paramname">wg_idx</td><td></td></tr>
    <tr><td class="paramname">wg_idy</td><td></td></tr>
    <tr><td class="paramname">sg_idx</td><td></td></tr>
    <tr><td class="paramname">sg_idy</td><td></td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd></dd></dl>

</div>
</div>
<a id="a451875db45fea17b9a83d2360a1289e5" name="a451875db45fea17b9a83d2360a1289e5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a451875db45fea17b9a83d2360a1289e5">&#9670;&#160;</a></span>post_op()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="common_2core_2common_8hpp.html#a9ed53999886ec13b86a4fe2e0fc16765">__XETLA_API</a> <a class="el" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8374ffb1cd05109a508506433904c0df">dtype_acc</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#abbad839064868aeea3ac306b17756ccd">chunk_size</a> &gt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::post_op </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8374ffb1cd05109a508506433904c0df">dtype_acc</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#abbad839064868aeea3ac306b17756ccd">chunk_size</a> &gt;&#160;</td>
          <td class="paramname"><em>input</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td></td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd></dd></dl>

</div>
</div>
<a id="af7cf66d03999e43e03066945a2dd8e56" name="af7cf66d03999e43e03066945a2dd8e56"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af7cf66d03999e43e03066945a2dd8e56">&#9670;&#160;</a></span>pre_op()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="common_2core_2common_8hpp.html#a9ed53999886ec13b86a4fe2e0fc16765">__XETLA_API</a> <a class="el" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8374ffb1cd05109a508506433904c0df">dtype_acc</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#abbad839064868aeea3ac306b17756ccd">chunk_size</a> &gt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::pre_op </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector</a>&lt; <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a8374ffb1cd05109a508506433904c0df">dtype_acc</a>, <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#abbad839064868aeea3ac306b17756ccd">chunk_size</a> &gt;&#160;</td>
          <td class="paramname"><em>input</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td></td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd></dd></dl>

</div>
</div>
<h2 class="groupheader">Member Data Documentation</h2>
<a id="ab290fcd351f7b2d194c2bb93e0d10dd5" name="ab290fcd351f7b2d194c2bb93e0d10dd5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab290fcd351f7b2d194c2bb93e0d10dd5">&#9670;&#160;</a></span>bias_dropout_res_out</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a50d4e962439f2a2f7bb9568c7bf68d11">bias_dropout_res_out_t</a> <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::bias_dropout_res_out</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a324d69157219c9f25037cf688efdf3c8" name="a324d69157219c9f25037cf688efdf3c8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a324d69157219c9f25037cf688efdf3c8">&#9670;&#160;</a></span>bias_dropout_res_out_payload</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a9d0578e62f9e29c083f6eeb2bf72f9d9">bias_dropout_res_out_payload_t</a> <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::bias_dropout_res_out_payload</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="af7df39edf03895c9b5447dd702bfe0ee" name="af7df39edf03895c9b5447dd702bfe0ee"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af7df39edf03895c9b5447dd702bfe0ee">&#9670;&#160;</a></span>bias_in</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a9f71fb5bb23b52eeae10a27610be999f">bias_in_t</a> <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::bias_in</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a8723833ccf23122c470f0919bf2105bd" name="a8723833ccf23122c470f0919bf2105bd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8723833ccf23122c470f0919bf2105bd">&#9670;&#160;</a></span>bias_in_payload</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a80e449100ea335a7771f182b692a6e30">bias_in_payload_t</a> <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::bias_in_payload</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="abbad839064868aeea3ac306b17756ccd" name="abbad839064868aeea3ac306b17756ccd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abbad839064868aeea3ac306b17756ccd">&#9670;&#160;</a></span>chunk_size</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">constexpr uint32_t <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::chunk_size = layer_norm_attr_::chunk_size</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">constexpr</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a041ae1afaf0a12ba2a800120ad2fa6e5" name="a041ae1afaf0a12ba2a800120ad2fa6e5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a041ae1afaf0a12ba2a800120ad2fa6e5">&#9670;&#160;</a></span>dropout_prob</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">float <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::dropout_prob</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a64b47183bccbc1882750636d92a8bcf6" name="a64b47183bccbc1882750636d92a8bcf6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a64b47183bccbc1882750636d92a8bcf6">&#9670;&#160;</a></span>dropout_scale</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">float <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::dropout_scale</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a2fbc396287e5c4333e17d5691bfa60d1" name="a2fbc396287e5c4333e17d5691bfa60d1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2fbc396287e5c4333e17d5691bfa60d1">&#9670;&#160;</a></span>fused_op_kind</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">constexpr <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7ec">ln_fwd_fused_kind</a> <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::fused_op_kind = <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">constexpr</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a6d497418df492c5a4a98c772001522e6" name="a6d497418df492c5a4a98c772001522e6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6d497418df492c5a4a98c772001522e6">&#9670;&#160;</a></span>mask_in</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a2d7ff9f0cc50829c059ee7736fab7a8e">mask_in_t</a> <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::mask_in</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a115d5ce3e8d492d4aa2c624432c2dc6d" name="a115d5ce3e8d492d4aa2c624432c2dc6d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a115d5ce3e8d492d4aa2c624432c2dc6d">&#9670;&#160;</a></span>mask_in_payload</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a531394b46ed357f0feb05eb8800ca5f9">mask_in_payload_t</a> <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::mask_in_payload</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a584f628ad8e0adb2f1e5723539e4ce56" name="a584f628ad8e0adb2f1e5723539e4ce56"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a584f628ad8e0adb2f1e5723539e4ce56">&#9670;&#160;</a></span>mask_ld</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">uint32_t <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::mask_ld</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a74cf23973a27bac115cc6e64af12d061" name="a74cf23973a27bac115cc6e64af12d061"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a74cf23973a27bac115cc6e64af12d061">&#9670;&#160;</a></span>mat_ld</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">uint32_t <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::mat_ld</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a8801d22bdbed500c4d5c15af0cbefd6c" name="a8801d22bdbed500c4d5c15af0cbefd6c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8801d22bdbed500c4d5c15af0cbefd6c">&#9670;&#160;</a></span>matrix_m</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">uint32_t <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::matrix_m</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a2730dc5052431571c642dc08245d758d" name="a2730dc5052431571c642dc08245d758d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2730dc5052431571c642dc08245d758d">&#9670;&#160;</a></span>matrix_n</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">uint32_t <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::matrix_n</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a9334902ba86ffc5c7768e459722ac72c" name="a9334902ba86ffc5c7768e459722ac72c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9334902ba86ffc5c7768e459722ac72c">&#9670;&#160;</a></span>n_chunks</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">constexpr uint32_t <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::n_chunks = <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a686ec6a547e37f87d42a046aaabbef8b">sg_tile_n</a> / <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#abbad839064868aeea3ac306b17756ccd">chunk_size</a></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">constexpr</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a308f91070f0b7badd144655e71150506" name="a308f91070f0b7badd144655e71150506"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a308f91070f0b7badd144655e71150506">&#9670;&#160;</a></span>res_in</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a4fc418cfed7e7c2139ba5a9c0c34e9a7">res_in_t</a> <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::res_in</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a70e0ba488891742486a946d9a96572b3" name="a70e0ba488891742486a946d9a96572b3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a70e0ba488891742486a946d9a96572b3">&#9670;&#160;</a></span>res_in_payload</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a7dc1d123523470abb2cf7b9219790bc6">res_in_payload_t</a> <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::res_in_payload</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a756569efe5da843eb01bcd9dc7ea4573" name="a756569efe5da843eb01bcd9dc7ea4573"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a756569efe5da843eb01bcd9dc7ea4573">&#9670;&#160;</a></span>sg_tile_m</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">constexpr uint32_t <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::sg_tile_m = layer_norm_attr_::sg_tile_m</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">constexpr</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a686ec6a547e37f87d42a046aaabbef8b" name="a686ec6a547e37f87d42a046aaabbef8b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a686ec6a547e37f87d42a046aaabbef8b">&#9670;&#160;</a></span>sg_tile_n</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">constexpr uint32_t <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::sg_tile_n = layer_norm_attr_::sg_tile_n</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">constexpr</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a5c72cd21e83ad0999a8b5e684748c414" name="a5c72cd21e83ad0999a8b5e684748c414"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5c72cd21e83ad0999a8b5e684748c414">&#9670;&#160;</a></span>wg_num_m</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">constexpr uint32_t <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::wg_num_m = layer_norm_attr_::wg_num_m</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">constexpr</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a49110b26a2b2afe3b1e2ea65d2cf5187" name="a49110b26a2b2afe3b1e2ea65d2cf5187"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a49110b26a2b2afe3b1e2ea65d2cf5187">&#9670;&#160;</a></span>wg_num_n</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">constexpr uint32_t <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::wg_num_n = layer_norm_attr_::wg_num_n</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">constexpr</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a96cb414a45a6c769e7a6c4347e764e36" name="a96cb414a45a6c769e7a6c4347e764e36"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a96cb414a45a6c769e7a6c4347e764e36">&#9670;&#160;</a></span>wg_size_x</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">constexpr uint32_t <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::wg_size_x = (<a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#aa6da3ba31fa9091675265e2157ce9437">wg_tile_n</a> + <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a686ec6a547e37f87d42a046aaabbef8b">sg_tile_n</a> - 1) / <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a686ec6a547e37f87d42a046aaabbef8b">sg_tile_n</a></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">constexpr</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a291a57aaf1f649714e9351f309e8cff7" name="a291a57aaf1f649714e9351f309e8cff7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a291a57aaf1f649714e9351f309e8cff7">&#9670;&#160;</a></span>wg_size_y</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">constexpr uint32_t <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::wg_size_y = (<a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a2c4b1f85f8d84ba460329ada6f660450">wg_tile_m</a> + <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a756569efe5da843eb01bcd9dc7ea4573">sg_tile_m</a> - 1) / <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html#a756569efe5da843eb01bcd9dc7ea4573">sg_tile_m</a></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">constexpr</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a2c4b1f85f8d84ba460329ada6f660450" name="a2c4b1f85f8d84ba460329ada6f660450"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2c4b1f85f8d84ba460329ada6f660450">&#9670;&#160;</a></span>wg_tile_m</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">constexpr uint32_t <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::wg_tile_m = layer_norm_attr_::wg_tile_m</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">constexpr</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="aa6da3ba31fa9091675265e2157ce9437" name="aa6da3ba31fa9091675265e2157ce9437"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa6da3ba31fa9091675265e2157ce9437">&#9670;&#160;</a></span>wg_tile_n</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename dtype_in_ , typename dtype_out_ , typename dtype_acc_ , typename layer_norm_attr_ &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">constexpr uint32_t <a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t.html">gpu::xetla::group::ln_fwd_fused_op_t</a>&lt; <a class="el" href="namespacegpu_1_1xetla.html#a71633e89354f0f1d0badcd0d90dea7eca50054c633c718e5b1c128c96ba1853ee">ln_fwd_fused_kind::bias_dropout_resAdd_ln</a>, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, <a class="el" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a> &gt;::wg_tile_n = layer_norm_attr_::wg_tile_n</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">constexpr</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespacegpu.html">gpu</a></li><li class="navelem"><a class="el" href="namespacegpu_1_1xetla.html">xetla</a></li><li class="navelem"><a class="el" href="namespacegpu_1_1xetla_1_1group.html">group</a></li><li class="navelem"><a class="el" href="structgpu_1_1xetla_1_1group_1_1ln__fwd__fused__op__t_3_01ln__fwd__fused__kind_1_1bias__dropout__7bd6ac5d5274c089bfc0bfda44480c4d.html">ln_fwd_fused_op_t&lt; ln_fwd_fused_kind::bias_dropout_resAdd_ln, dtype_in_, dtype_out_, dtype_acc_, layer_norm_attr_, gpu_arch::Xe &gt;</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5 </li>
  </ul>
</div>
</body>
</html>
