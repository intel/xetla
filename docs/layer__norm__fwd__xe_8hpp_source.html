<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>XeTLA: include/experimental/kernel/layer_norm/layer_norm_fwd_xe.hpp Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">XeTLA<span id="projectnumber">&#160;v0.3.3</span>
   </div>
   <div id="projectbrief">IntelÂ® Xe Templates for Linear Algebra - API Definition Document</div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('layer__norm__fwd__xe_8hpp_source.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="headertitle"><div class="title">layer_norm_fwd_xe.hpp</div></div>
</div><!--header-->
<div class="contents">
<a href="layer__norm__fwd__xe_8hpp.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a id="l00001" name="l00001"></a><span class="lineno">    1</span><span class="comment">/*******************************************************************************</span></div>
<div class="line"><a id="l00002" name="l00002"></a><span class="lineno">    2</span><span class="comment">* Copyright (c) 2022-2023 Intel Corporation</span></div>
<div class="line"><a id="l00003" name="l00003"></a><span class="lineno">    3</span><span class="comment">*</span></div>
<div class="line"><a id="l00004" name="l00004"></a><span class="lineno">    4</span><span class="comment">* Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></div>
<div class="line"><a id="l00005" name="l00005"></a><span class="lineno">    5</span><span class="comment">* you may not use this file except in compliance with the License.</span></div>
<div class="line"><a id="l00006" name="l00006"></a><span class="lineno">    6</span><span class="comment">* You may obtain a copy of the License at</span></div>
<div class="line"><a id="l00007" name="l00007"></a><span class="lineno">    7</span><span class="comment">*</span></div>
<div class="line"><a id="l00008" name="l00008"></a><span class="lineno">    8</span><span class="comment">*     http://www.apache.org/licenses/LICENSE-2.0</span></div>
<div class="line"><a id="l00009" name="l00009"></a><span class="lineno">    9</span><span class="comment">*</span></div>
<div class="line"><a id="l00010" name="l00010"></a><span class="lineno">   10</span><span class="comment">* Unless required by applicable law or agreed to in writing, software</span></div>
<div class="line"><a id="l00011" name="l00011"></a><span class="lineno">   11</span><span class="comment">* distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></div>
<div class="line"><a id="l00012" name="l00012"></a><span class="lineno">   12</span><span class="comment">* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></div>
<div class="line"><a id="l00013" name="l00013"></a><span class="lineno">   13</span><span class="comment">* See the License for the specific language governing permissions and</span></div>
<div class="line"><a id="l00014" name="l00014"></a><span class="lineno">   14</span><span class="comment">* limitations under the License.</span></div>
<div class="line"><a id="l00015" name="l00015"></a><span class="lineno">   15</span><span class="comment">*******************************************************************************/</span></div>
<div class="line"><a id="l00016" name="l00016"></a><span class="lineno">   16</span> </div>
<div class="line"><a id="l00019" name="l00019"></a><span class="lineno">   19</span> </div>
<div class="line"><a id="l00020" name="l00020"></a><span class="lineno">   20</span><span class="preprocessor">#pragma once</span></div>
<div class="line"><a id="l00021" name="l00021"></a><span class="lineno">   21</span> </div>
<div class="line"><a id="l00022" name="l00022"></a><span class="lineno">   22</span><span class="preprocessor">#include &quot;<a class="code" href="layer__norm__fused__op__fwd__xe_8hpp.html">experimental/group/fused_op/layer_norm_fused_op_fwd_xe.hpp</a>&quot;</span></div>
<div class="line"><a id="l00023" name="l00023"></a><span class="lineno">   23</span><span class="preprocessor">#include &quot;<a class="code" href="experimental_2kernel_2layer__norm_2api_8hpp.html">experimental/kernel/layer_norm/api.hpp</a>&quot;</span></div>
<div class="line"><a id="l00024" name="l00024"></a><span class="lineno">   24</span><span class="preprocessor">#include &quot;<a class="code" href="experimental_2kernel_2layer__norm_2common_8hpp.html">experimental/kernel/layer_norm/common.hpp</a>&quot;</span></div>
<div class="line"><a id="l00025" name="l00025"></a><span class="lineno">   25</span><span class="preprocessor">#include &quot;<a class="code" href="layer__norm_2config_8hpp.html">experimental/kernel/layer_norm/config.hpp</a>&quot;</span></div>
<div class="line"><a id="l00026" name="l00026"></a><span class="lineno">   26</span> </div>
<div class="line"><a id="l00027" name="l00027"></a><span class="lineno">   27</span><span class="keyword">namespace </span><a class="code hl_namespace" href="namespacegpu_1_1xetla_1_1kernel.html">gpu::xetla::kernel</a> {</div>
<div class="line"><a id="l00028" name="l00028"></a><span class="lineno">   28</span> </div>
<div class="line"><a id="l00038" name="l00038"></a><span class="lineno">   38</span><span class="keyword">template</span> &lt;<span class="keyword">typename</span> dtype_x_, <span class="keyword">typename</span> dtype_y_, <span class="keyword">typename</span> dtype_weight_,</div>
<div class="line"><a id="l00039" name="l00039"></a><span class="lineno">   39</span>        <span class="keyword">typename</span> dtype_acc_, <span class="keyword">typename</span> layer_norm_attr_, <span class="keywordtype">bool</span> store_for_bwd_,</div>
<div class="line"><a id="l00040" name="l00040"></a><span class="lineno">   40</span>        <span class="keyword">typename</span> ln_fwd_fused_op_&gt;</div>
<div class="line"><a id="l00041" name="l00041"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html">   41</a></span><span class="keyword">struct </span><a class="code hl_struct" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t.html">layer_norm_fwd_t</a>&lt;dtype_x_, dtype_y_, dtype_weight_, dtype_acc_,</div>
<div class="line"><a id="l00042" name="l00042"></a><span class="lineno">   42</span>        layer_norm_attr_, store_for_bwd_, <a class="code hl_enumeration" href="group__xetla__core__arch__config.html#gaa5a2713edb27d6fed88a3c61673556f1">gpu_arch</a>::<a class="code hl_enumvalue" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">Xe</a>, ln_fwd_fused_op_&gt; {</div>
<div class="line"><a id="l00043" name="l00043"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a301cef1451b18ca22bf1ee2fddc59889">   43</a></span>    <span class="keyword">using </span><a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a301cef1451b18ca22bf1ee2fddc59889">dtype_x</a> = dtype_x_;</div>
<div class="line"><a id="l00044" name="l00044"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a77e9ac25570625f11b5de2de9e63e781">   44</a></span>    <span class="keyword">using </span><a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a77e9ac25570625f11b5de2de9e63e781">dtype_y</a> = dtype_y_;</div>
<div class="line"><a id="l00045" name="l00045"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#afabd0691dfb4592ec312cff8b8b46767">   45</a></span>    <span class="keyword">using </span><a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#afabd0691dfb4592ec312cff8b8b46767">dtype_weight</a> = dtype_weight_;</div>
<div class="line"><a id="l00046" name="l00046"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">   46</a></span>    <span class="keyword">using </span><a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a> = dtype_acc_;</div>
<div class="line"><a id="l00047" name="l00047"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a16904944686fbfe4b4ece2ea5753491d">   47</a></span>    <span class="keyword">using </span><a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a16904944686fbfe4b4ece2ea5753491d">layer_norm_attr</a> = layer_norm_attr_;</div>
<div class="line"><a id="l00048" name="l00048"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a9ec85363866af435edc21470ca16802d">   48</a></span>    <span class="keyword">using </span><a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a9ec85363866af435edc21470ca16802d">ln_fwd_fused_op</a> = ln_fwd_fused_op_;</div>
<div class="line"><a id="l00049" name="l00049"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#aed6a9be25e7ad6eac7563d4a70166709">   49</a></span>    <span class="keyword">using </span><a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#aed6a9be25e7ad6eac7563d4a70166709">ln_fused_op_arguments_t</a> = <span class="keyword">typename</span> ln_fwd_fused_op::arguments_t;</div>
<div class="line"><a id="l00050" name="l00050"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a29a8459806d4b3094ae3a7f15f893242">   50</a></span>    <span class="keyword">static</span> <span class="keyword">constexpr</span> <span class="keywordtype">bool</span> store_for_bwd = store_for_bwd_;</div>
<div class="line"><a id="l00051" name="l00051"></a><span class="lineno">   51</span> </div>
<div class="line"><a id="l00052" name="l00052"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a2defbac994f82cab923e377011a83c10">   52</a></span>    <span class="keyword">static</span> <span class="keyword">constexpr</span> uint32_t wg_tile_m = layer_norm_attr::wg_tile_m;</div>
<div class="line"><a id="l00053" name="l00053"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#acac75ebb6033e9a11f83144a533191f4">   53</a></span>    <span class="keyword">static</span> <span class="keyword">constexpr</span> uint32_t wg_tile_n = layer_norm_attr::wg_tile_n;</div>
<div class="line"><a id="l00054" name="l00054"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a45b3dfbbe9656b9a51714e6f9fb987df">   54</a></span>    <span class="keyword">static</span> <span class="keyword">constexpr</span> uint32_t sg_tile_m = layer_norm_attr::sg_tile_m;</div>
<div class="line"><a id="l00055" name="l00055"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a564c096226f2f3e0a7b3c4bad454bece">   55</a></span>    <span class="keyword">static</span> <span class="keyword">constexpr</span> uint32_t sg_tile_n = layer_norm_attr::sg_tile_n;</div>
<div class="line"><a id="l00056" name="l00056"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a4a96d43080dd7fbfc2d372cbbf7368a7">   56</a></span>    <span class="keyword">static</span> <span class="keyword">constexpr</span> uint32_t wg_num_m = layer_norm_attr::wg_num_m;</div>
<div class="line"><a id="l00057" name="l00057"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a0bb3ec7fe898ee9f30cb622aeb364772">   57</a></span>    <span class="keyword">static</span> <span class="keyword">constexpr</span> uint32_t wg_num_n = layer_norm_attr::wg_num_n;</div>
<div class="line"><a id="l00058" name="l00058"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#aa7dc1fcb41216788e801e7c6a654559c">   58</a></span>    <span class="keyword">static</span> <span class="keyword">constexpr</span> uint32_t chunk_size = layer_norm_attr::chunk_size;</div>
<div class="line"><a id="l00059" name="l00059"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a6c40b24a0a65c40a1fb68d29edcfccd2">   59</a></span>    <span class="keyword">static</span> <span class="keyword">constexpr</span> uint32_t n_chunks = sg_tile_n / chunk_size;</div>
<div class="line"><a id="l00060" name="l00060"></a><span class="lineno">   60</span>    <span class="keyword">static_assert</span>(sg_tile_n % chunk_size == 0,</div>
<div class="line"><a id="l00061" name="l00061"></a><span class="lineno">   61</span>            <span class="stringliteral">&quot;Current impl does not support tailing mechanism&quot;</span>);</div>
<div class="line"><a id="l00062" name="l00062"></a><span class="lineno">   62</span> </div>
<div class="line"><a id="l00063" name="l00063"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#ab3d0864b6b402e4f7cf797fadd43ba54">   63</a></span>    <span class="keyword">static</span> <span class="keyword">constexpr</span> uint32_t wg_size_x</div>
<div class="line"><a id="l00064" name="l00064"></a><span class="lineno">   64</span>            = (wg_tile_n + sg_tile_n - 1) / sg_tile_n;</div>
<div class="line"><a id="l00065" name="l00065"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a90cd71aa5aac1f75b64f78ca6329b03c">   65</a></span>    <span class="keyword">static</span> <span class="keyword">constexpr</span> uint32_t wg_size_y</div>
<div class="line"><a id="l00066" name="l00066"></a><span class="lineno">   66</span>            = (wg_tile_m + sg_tile_m - 1) / sg_tile_m;</div>
<div class="line"><a id="l00067" name="l00067"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a5597dc4fa9d1e4bcd545741ce1d252b9">   67</a></span>    <span class="keyword">using </span><a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a5597dc4fa9d1e4bcd545741ce1d252b9">work_group_t</a> = <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a5597dc4fa9d1e4bcd545741ce1d252b9">work_group_t&lt;wg_size_x * wg_size_y&gt;</a>;</div>
<div class="line"><a id="l00068" name="l00068"></a><span class="lineno">   68</span>    <span class="keyword">static_assert</span>((wg_size_x &lt;= 32) &amp;&amp; ((wg_size_x &amp; (wg_size_x - 1)) == 0),</div>
<div class="line"><a id="l00069" name="l00069"></a><span class="lineno">   69</span>            <span class="stringliteral">&quot;Current only support wg_size_x &lt;=32&quot;</span>);</div>
<div class="line"><a id="l00070" name="l00070"></a><span class="lineno">   70</span> </div>
<div class="line"><a id="l00073" name="l00073"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wd08696fa56c321e029984c701d4f5730.html">   73</a></span>    <span class="keyword">struct </span>get_barrier_count {</div>
<div class="line"><a id="l00074" name="l00074"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wd08696fa56c321e029984c701d4f5730.html#ad9f27679c565dc6573d99df202655a4a">   74</a></span>        <span class="keyword">static</span> <span class="keyword">constexpr</span> uint32_t count = (wg_size_x &gt; 1) ? wg_size_y : 0;</div>
<div class="line"><a id="l00075" name="l00075"></a><span class="lineno">   75</span>    };</div>
<div class="line"><a id="l00076" name="l00076"></a><span class="lineno">   76</span> </div>
<div class="line"><a id="l00079" name="l00079"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w1fae1b1d7e31ebc935e4665707a2f860.html">   79</a></span>    <span class="keyword">struct </span>get_slm_size {</div>
<div class="line"><a id="l00080" name="l00080"></a><span class="lineno">   80</span>        <span class="comment">// 4 = (mu + m2) * double buffering</span></div>
<div class="line"><a id="l00081" name="l00081"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w1fae1b1d7e31ebc935e4665707a2f860.html#ad1036484a469c4f4b69cce714e074bb0">   81</a></span>        <span class="keyword">static</span> <span class="keyword">constexpr</span> uint32_t size = (wg_size_x &gt; 1)</div>
<div class="line"><a id="l00082" name="l00082"></a><span class="lineno">   82</span>                ? wg_size_x * wg_size_y * 4 * <span class="keyword">sizeof</span>(<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>)</div>
<div class="line"><a id="l00083" name="l00083"></a><span class="lineno">   83</span>                : 0;</div>
<div class="line"><a id="l00084" name="l00084"></a><span class="lineno">   84</span>    };</div>
<div class="line"><a id="l00085" name="l00085"></a><span class="lineno">   85</span> </div>
<div class="line"><a id="l00086" name="l00086"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a63640d22752acc609f91cd194cad7897">   86</a></span>    <span class="keyword">using </span><a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__desc__t.html">ln_fwd_tile_desc_t</a> = <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__desc__t.html">subgroup::tile_desc_t</a>&lt;chunk_size, 1, chunk_size,</div>
<div class="line"><a id="l00087" name="l00087"></a><span class="lineno">   87</span>            1, <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#a51137fd81d0d9d2156525a1e279432aaa78506882d645395a052df8b01a927395">reg_layout::tiled</a>&gt;;</div>
<div class="line"><a id="l00088" name="l00088"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a0a96240a4822bd097bc0ae17652795d3">   88</a></span>    <span class="keyword">using </span><a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">x_in_t</a> = <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">subgroup::tile_t&lt;dtype_x, ln_fwd_tile_desc_t&gt;</a>;</div>
<div class="line"><a id="l00089" name="l00089"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#af56a4c0e4d764cae7bd8bbe86dab9aec">   89</a></span>    <span class="keyword">using </span><a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">gamma_in_t</a> = <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">subgroup::tile_t&lt;dtype_weight, ln_fwd_tile_desc_t&gt;</a>;</div>
<div class="line"><a id="l00090" name="l00090"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a5570600f93eff0c8c81ec41b4b6f2336">   90</a></span>    <span class="keyword">using </span><a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">beta_in_t</a> = <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">subgroup::tile_t&lt;dtype_weight, ln_fwd_tile_desc_t&gt;</a>;</div>
<div class="line"><a id="l00091" name="l00091"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a4483049d2bbe6b71dd26b12d2904610b">   91</a></span>    <span class="keyword">using </span><a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">y_out_t</a> = <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">subgroup::tile_t&lt;dtype_y, ln_fwd_tile_desc_t&gt;</a>;</div>
<div class="line"><a id="l00092" name="l00092"></a><span class="lineno">   92</span> </div>
<div class="line"><a id="l00093" name="l00093"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a206d44cc337dd96c6aebd1f19d278d5d">   93</a></span>    <span class="keyword">using </span><a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">x_in_payload_t</a> = <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">subgroup::mem_payload_t</a>&lt;<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a301cef1451b18ca22bf1ee2fddc59889">dtype_x</a>, <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__desc__t.html">ln_fwd_tile_desc_t</a>,</div>
<div class="line"><a id="l00094" name="l00094"></a><span class="lineno">   94</span>            subgroup::msg_type_v&lt;ln_fwd_tile_desc_t, mem_space::global&gt;,</div>
<div class="line"><a id="l00095" name="l00095"></a><span class="lineno">   95</span>            <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#af4a355a1806510c5515fad16f5910561a641fabb8e5e7d1d0333e2c9c384f959c">mem_layout::row_major</a>, <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#a7f225ed816e841c1d31414d872dae59da9c70933aff6b2a6d08c687a6cbb6b765">mem_space::global</a>, <a class="code hl_enumvalue" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a>&gt;;</div>
<div class="line"><a id="l00096" name="l00096"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a18955e8731ef0974f4e7868e69797626">   96</a></span>    <span class="keyword">using </span><a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">gamma_in_payload_t</a></div>
<div class="line"><a id="l00097" name="l00097"></a><span class="lineno">   97</span>            = <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">subgroup::mem_payload_t</a>&lt;<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#afabd0691dfb4592ec312cff8b8b46767">dtype_weight</a>, <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__desc__t.html">ln_fwd_tile_desc_t</a>,</div>
<div class="line"><a id="l00098" name="l00098"></a><span class="lineno">   98</span>                    subgroup::msg_type_v&lt;ln_fwd_tile_desc_t, mem_space::global&gt;,</div>
<div class="line"><a id="l00099" name="l00099"></a><span class="lineno">   99</span>                    <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#af4a355a1806510c5515fad16f5910561a641fabb8e5e7d1d0333e2c9c384f959c">mem_layout::row_major</a>, <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#a7f225ed816e841c1d31414d872dae59da9c70933aff6b2a6d08c687a6cbb6b765">mem_space::global</a>, <a class="code hl_enumvalue" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a>&gt;;</div>
<div class="line"><a id="l00100" name="l00100"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#aa1c70083fb30b2ca88cbb7eeaa3c8f1c">  100</a></span>    <span class="keyword">using </span><a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">beta_in_payload_t</a></div>
<div class="line"><a id="l00101" name="l00101"></a><span class="lineno">  101</span>            = <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">subgroup::mem_payload_t</a>&lt;<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#afabd0691dfb4592ec312cff8b8b46767">dtype_weight</a>, <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__desc__t.html">ln_fwd_tile_desc_t</a>,</div>
<div class="line"><a id="l00102" name="l00102"></a><span class="lineno">  102</span>                    subgroup::msg_type_v&lt;ln_fwd_tile_desc_t, mem_space::global&gt;,</div>
<div class="line"><a id="l00103" name="l00103"></a><span class="lineno">  103</span>                    <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#af4a355a1806510c5515fad16f5910561a641fabb8e5e7d1d0333e2c9c384f959c">mem_layout::row_major</a>, <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#a7f225ed816e841c1d31414d872dae59da9c70933aff6b2a6d08c687a6cbb6b765">mem_space::global</a>, <a class="code hl_enumvalue" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a>&gt;;</div>
<div class="line"><a id="l00104" name="l00104"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a641a9b058b367f41cc2fd3b35e9b24fb">  104</a></span>    <span class="keyword">using </span><a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">y_out_payload_t</a> = <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">subgroup::mem_payload_t</a>&lt;<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a77e9ac25570625f11b5de2de9e63e781">dtype_y</a>, <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__desc__t.html">ln_fwd_tile_desc_t</a>,</div>
<div class="line"><a id="l00105" name="l00105"></a><span class="lineno">  105</span>            <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#aa8afe1d12e7777419fb6ea09534a0aa7a5baf9a1aba8e4b6fa44c20aafacc4233">msg_type::block_1d</a>, <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#af4a355a1806510c5515fad16f5910561a641fabb8e5e7d1d0333e2c9c384f959c">mem_layout::row_major</a>, <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#a7f225ed816e841c1d31414d872dae59da9c70933aff6b2a6d08c687a6cbb6b765">mem_space::global</a>,</div>
<div class="line"><a id="l00106" name="l00106"></a><span class="lineno">  106</span>            <a class="code hl_enumvalue" href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu_arch::Xe</a>&gt;;</div>
<div class="line"><a id="l00107" name="l00107"></a><span class="lineno">  107</span> </div>
<div class="line"><a id="l00110" name="l00110"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html">  110</a></span>    <span class="keyword">struct </span>arguments_t {</div>
<div class="line"><a id="l00111" name="l00111"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#abaa8295ca8928c30930ff501215f1142">  111</a></span>        <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a301cef1451b18ca22bf1ee2fddc59889">dtype_x</a> *<a class="code hl_variable" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#abaa8295ca8928c30930ff501215f1142">x_in_ptr</a>;</div>
<div class="line"><a id="l00112" name="l00112"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#aeedc6bd926f640e1ca431d76559cb11b">  112</a></span>        <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#afabd0691dfb4592ec312cff8b8b46767">dtype_weight</a> *<a class="code hl_variable" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#aeedc6bd926f640e1ca431d76559cb11b">gamma_ptr</a>;</div>
<div class="line"><a id="l00113" name="l00113"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#abd672da166c6339a297a24338d8bfcbc">  113</a></span>        <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#afabd0691dfb4592ec312cff8b8b46767">dtype_weight</a> *<a class="code hl_variable" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#abd672da166c6339a297a24338d8bfcbc">beta_ptr</a>;</div>
<div class="line"><a id="l00114" name="l00114"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#aa808e348768e4008a264c9af99e995ca">  114</a></span>        <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a77e9ac25570625f11b5de2de9e63e781">dtype_y</a> *<a class="code hl_variable" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#aa808e348768e4008a264c9af99e995ca">y_out_ptr</a>;</div>
<div class="line"><a id="l00115" name="l00115"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#ab20a6582052f44dc57210137461b9b87">  115</a></span>        <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a> *<a class="code hl_variable" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#ab20a6582052f44dc57210137461b9b87">rs_ptr</a>;</div>
<div class="line"><a id="l00116" name="l00116"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#af7e516fb6543029649d2b79f1e59c052">  116</a></span>        <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a> *<a class="code hl_variable" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#af7e516fb6543029649d2b79f1e59c052">mu_ptr</a>;</div>
<div class="line"><a id="l00117" name="l00117"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#a2ea50368a4261455d64b75376c19295d">  117</a></span>        uint32_t <a class="code hl_variable" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#a2ea50368a4261455d64b75376c19295d">matrix_m</a>;</div>
<div class="line"><a id="l00118" name="l00118"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#a1390eb14f00da2b114da33efbfe2f2e1">  118</a></span>        uint32_t <a class="code hl_variable" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#a1390eb14f00da2b114da33efbfe2f2e1">matrix_n</a>;</div>
<div class="line"><a id="l00119" name="l00119"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#a576f7319fa331610b05b580ca0432841">  119</a></span>        uint32_t <a class="code hl_variable" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#a576f7319fa331610b05b580ca0432841">mat_ld</a>;</div>
<div class="line"><a id="l00120" name="l00120"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#a379d3d8c4b86099a2b1ae55b275c0c9d">  120</a></span>        <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a> epsilon = 1e-5;</div>
<div class="line"><a id="l00121" name="l00121"></a><span class="lineno">  121</span>    };</div>
<div class="line"><a id="l00122" name="l00122"></a><span class="lineno">  122</span> </div>
<div class="line"><a id="l00128" name="l00128"></a><span class="lineno">  128</span>    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, u<span class="keywordtype">int</span>32_t SZ, u<span class="keywordtype">int</span>32_t N&gt;</div>
<div class="line"><a id="l00129" name="l00129"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w1874b19073e1f423699cbfc0c66a9c60.html">  129</a></span>    <span class="keyword">struct </span>parallel_mu_m2_t {</div>
<div class="line"><a id="l00130" name="l00130"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w1874b19073e1f423699cbfc0c66a9c60.html#af7efa4be4adc4580d377e4ca7b15e95a">  130</a></span>        <span class="keyword">static</span> <span class="keyword">inline</span> <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;T, 2&gt;</a> <a class="code hl_function" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w1874b19073e1f423699cbfc0c66a9c60.html#af7efa4be4adc4580d377e4ca7b15e95a">call</a>(</div>
<div class="line"><a id="l00131" name="l00131"></a><span class="lineno">  131</span>                <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;T, SZ&gt;</a> mu_vec, <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;T, SZ&gt;</a> m2_vec) {</div>
<div class="line"><a id="l00132" name="l00132"></a><span class="lineno">  132</span>            <span class="keyword">auto</span> mu_vec_a = mu_vec.xetla_select&lt;SZ / 2, 1&gt;(0);</div>
<div class="line"><a id="l00133" name="l00133"></a><span class="lineno">  133</span>            <span class="keyword">auto</span> mu_vec_b = mu_vec.xetla_select&lt;SZ / 2, 1&gt;(SZ / 2);</div>
<div class="line"><a id="l00134" name="l00134"></a><span class="lineno">  134</span>            <span class="keyword">auto</span> m2_vec_a = m2_vec.xetla_select&lt;SZ / 2, 1&gt;(0);</div>
<div class="line"><a id="l00135" name="l00135"></a><span class="lineno">  135</span>            <span class="keyword">auto</span> m2_vec_b = m2_vec.xetla_select&lt;SZ / 2, 1&gt;(SZ / 2);</div>
<div class="line"><a id="l00136" name="l00136"></a><span class="lineno">  136</span>            <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector</a>&lt;T, SZ / 2&gt; mu_vec_new = (mu_vec_a + mu_vec_b) / (T)2;</div>
<div class="line"><a id="l00137" name="l00137"></a><span class="lineno">  137</span>            <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector</a>&lt;T, SZ / 2&gt; m2_vec_new = m2_vec_a + m2_vec_b</div>
<div class="line"><a id="l00138" name="l00138"></a><span class="lineno">  138</span>                    + (mu_vec_a - mu_vec_b) * (mu_vec_a - mu_vec_b) * (T)N</div>
<div class="line"><a id="l00139" name="l00139"></a><span class="lineno">  139</span>                            / (T)2;</div>
<div class="line"><a id="l00140" name="l00140"></a><span class="lineno">  140</span>            <span class="keywordflow">return</span> parallel_mu_m2_t&lt;T, SZ / 2, N * 2&gt;::call(</div>
<div class="line"><a id="l00141" name="l00141"></a><span class="lineno">  141</span>                    mu_vec_new, m2_vec_new);</div>
<div class="line"><a id="l00142" name="l00142"></a><span class="lineno">  142</span>        }</div>
<div class="line"><a id="l00143" name="l00143"></a><span class="lineno">  143</span>    };</div>
<div class="line"><a id="l00144" name="l00144"></a><span class="lineno">  144</span> </div>
<div class="line"><a id="l00149" name="l00149"></a><span class="lineno">  149</span>    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, u<span class="keywordtype">int</span>32_t N&gt;</div>
<div class="line"><a id="l00150" name="l00150"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w34b86ab2795b16851d713a3deae4a27a.html">  150</a></span>    <span class="keyword">struct </span>parallel_mu_m2_t&lt;T, 1, N&gt; {</div>
<div class="line"><a id="l00151" name="l00151"></a><span class="lineno">  151</span> </div>
<div class="line"><a id="l00157" name="l00157"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w34b86ab2795b16851d713a3deae4a27a.html#a03880fd66a722a0d96638ac4bca9604e">  157</a></span>        <span class="keyword">static</span> <span class="keyword">inline</span> <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;T, 2&gt;</a> <a class="code hl_function" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w34b86ab2795b16851d713a3deae4a27a.html#a03880fd66a722a0d96638ac4bca9604e">call</a>(</div>
<div class="line"><a id="l00158" name="l00158"></a><span class="lineno">  158</span>                <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;T, 1&gt;</a> mu_vec, <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;T, 1&gt;</a> m2_vec) {</div>
<div class="line"><a id="l00159" name="l00159"></a><span class="lineno">  159</span>            <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;T, 2&gt;</a> ret;</div>
<div class="line"><a id="l00160" name="l00160"></a><span class="lineno">  160</span>            ret[0] = mu_vec[0];</div>
<div class="line"><a id="l00161" name="l00161"></a><span class="lineno">  161</span>            ret[1] = m2_vec[0];</div>
<div class="line"><a id="l00162" name="l00162"></a><span class="lineno">  162</span>            <span class="keywordflow">return</span> ret;</div>
<div class="line"><a id="l00163" name="l00163"></a><span class="lineno">  163</span>        }</div>
<div class="line"><a id="l00164" name="l00164"></a><span class="lineno">  164</span>    };</div>
<div class="line"><a id="l00165" name="l00165"></a><span class="lineno">  165</span> </div>
<div class="line"><a id="l00174" name="l00174"></a><span class="lineno"><a class="line" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a1f8407b53863be4dd2e20aaf4f534508">  174</a></span>    <a class="code hl_define" href="common_2core_2common_8hpp.html#a9ed53999886ec13b86a4fe2e0fc16765">__XETLA_API</a> <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code hl_function" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a1f8407b53863be4dd2e20aaf4f534508">call</a>(<a class="code hl_class" href="classgpu_1_1xetla_1_1xetla__exec__item.html">xetla_exec_item&lt;3&gt;</a> &amp;ei, arguments_t *args,</div>
<div class="line"><a id="l00175" name="l00175"></a><span class="lineno">  175</span>            uint32_t slm_base = 0, uint32_t nbarrier_base = 0,</div>
<div class="line"><a id="l00176" name="l00176"></a><span class="lineno">  176</span>            <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#aed6a9be25e7ad6eac7563d4a70166709">ln_fused_op_arguments_t</a> *fused_op_args = <span class="keyword">nullptr</span>) {</div>
<div class="line"><a id="l00177" name="l00177"></a><span class="lineno">  177</span>        <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a5597dc4fa9d1e4bcd545741ce1d252b9">work_group_t</a> g;</div>
<div class="line"><a id="l00178" name="l00178"></a><span class="lineno">  178</span>        g.init(ei.<a class="code hl_function" href="classgpu_1_1xetla_1_1xetla__exec__item.html#a740f63e18f312690bfae48783da27644">get_local_linear_id</a>());</div>
<div class="line"><a id="l00179" name="l00179"></a><span class="lineno">  179</span>        <span class="keywordtype">int</span> sg_idx = g.get_id() % wg_size_x;</div>
<div class="line"><a id="l00180" name="l00180"></a><span class="lineno">  180</span>        <span class="keywordtype">int</span> sg_idy = g.get_id() / wg_size_x;</div>
<div class="line"><a id="l00181" name="l00181"></a><span class="lineno">  181</span>        <span class="keywordtype">int</span> wg_idx = ei.<a class="code hl_function" href="classgpu_1_1xetla_1_1xetla__exec__item.html#ac46733b57cc482439c9f7e13916c1514">get_group</a>(2);</div>
<div class="line"><a id="l00182" name="l00182"></a><span class="lineno">  182</span>        <span class="keywordtype">int</span> wg_idy = ei.<a class="code hl_function" href="classgpu_1_1xetla_1_1xetla__exec__item.html#ac46733b57cc482439c9f7e13916c1514">get_group</a>(1);</div>
<div class="line"><a id="l00183" name="l00183"></a><span class="lineno">  183</span>        <span class="keywordtype">int</span> start_n = wg_idx * wg_tile_n + sg_idx * sg_tile_n;</div>
<div class="line"><a id="l00184" name="l00184"></a><span class="lineno">  184</span>        <span class="keywordtype">int</span> start_m = wg_idy * wg_tile_m + sg_idy * sg_tile_m;</div>
<div class="line"><a id="l00185" name="l00185"></a><span class="lineno">  185</span> </div>
<div class="line"><a id="l00186" name="l00186"></a><span class="lineno">  186</span>        <a class="code hl_struct" href="structgpu_1_1xetla_1_1xetla__nbarrier__t.html">xetla_nbarrier_t&lt;wg_size_x, wg_size_x&gt;</a> nbarrier;</div>
<div class="line"><a id="l00187" name="l00187"></a><span class="lineno">  187</span>        nbarrier.<a class="code hl_function" href="structgpu_1_1xetla_1_1xetla__nbarrier__t.html#ae77b04d911300020ab3dff5f9d4f9d54">init_nbarrier</a>(</div>
<div class="line"><a id="l00188" name="l00188"></a><span class="lineno">  188</span>                sg_idy + nbarrier_base, <a class="code hl_enumvalue" href="group__xetla__util__named__barrier.html#ggaad562941909edcd788dd091e31841fe1a4c44b0011c9c00f486a2d65f1293de8f">nbarrier_role::producer_consumer</a>);</div>
<div class="line"><a id="l00189" name="l00189"></a><span class="lineno">  189</span> </div>
<div class="line"><a id="l00190" name="l00190"></a><span class="lineno">  190</span>        <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">x_in_t</a> x_in;</div>
<div class="line"><a id="l00191" name="l00191"></a><span class="lineno">  191</span>        <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">x_in_payload_t</a> x_in_payload;</div>
<div class="line"><a id="l00192" name="l00192"></a><span class="lineno">  192</span>        <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">gamma_in_t</a> gamma_in;</div>
<div class="line"><a id="l00193" name="l00193"></a><span class="lineno">  193</span>        <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">gamma_in_payload_t</a> gamma_in_payload;</div>
<div class="line"><a id="l00194" name="l00194"></a><span class="lineno">  194</span>        <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">beta_in_t</a> beta_in;</div>
<div class="line"><a id="l00195" name="l00195"></a><span class="lineno">  195</span>        <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">beta_in_payload_t</a> beta_in_payload;</div>
<div class="line"><a id="l00196" name="l00196"></a><span class="lineno">  196</span>        <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">y_out_t</a> y_out;</div>
<div class="line"><a id="l00197" name="l00197"></a><span class="lineno">  197</span>        <a class="code hl_struct" href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">y_out_payload_t</a> y_out_payload;</div>
<div class="line"><a id="l00198" name="l00198"></a><span class="lineno">  198</span>        <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a9ec85363866af435edc21470ca16802d">ln_fwd_fused_op</a> fused_op;</div>
<div class="line"><a id="l00199" name="l00199"></a><span class="lineno">  199</span>        x_in_payload.init(args-&gt;x_in_ptr, args-&gt;matrix_n, args-&gt;matrix_m,</div>
<div class="line"><a id="l00200" name="l00200"></a><span class="lineno">  200</span>                args-&gt;mat_ld, start_n, start_m);</div>
<div class="line"><a id="l00201" name="l00201"></a><span class="lineno">  201</span>        <span class="comment">// &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; fused op fwd init</span></div>
<div class="line"><a id="l00202" name="l00202"></a><span class="lineno">  202</span> </div>
<div class="line"><a id="l00203" name="l00203"></a><span class="lineno">  203</span>        <span class="keywordflow">if</span> <span class="keyword">constexpr</span> (n_chunks == 1) {</div>
<div class="line"><a id="l00204" name="l00204"></a><span class="lineno">  204</span>            fused_op.init(</div>
<div class="line"><a id="l00205" name="l00205"></a><span class="lineno">  205</span>                    fused_op_args, wg_idx, wg_idy, sg_idx, sg_idy, start_m);</div>
<div class="line"><a id="l00206" name="l00206"></a><span class="lineno">  206</span>            gamma_in_payload.init(args-&gt;gamma_ptr, args-&gt;matrix_n, 1,</div>
<div class="line"><a id="l00207" name="l00207"></a><span class="lineno">  207</span>                    args-&gt;mat_ld, start_n, 0);</div>
<div class="line"><a id="l00208" name="l00208"></a><span class="lineno">  208</span>            beta_in_payload.init(args-&gt;beta_ptr, args-&gt;matrix_n, 1,</div>
<div class="line"><a id="l00209" name="l00209"></a><span class="lineno">  209</span>                    args-&gt;mat_ld, start_n, 0);</div>
<div class="line"><a id="l00210" name="l00210"></a><span class="lineno">  210</span>            <a class="code hl_function" href="namespacegpu_1_1xetla_1_1subgroup.html#ae110043dd5332ce2a7ef91d2c8ae79ef">subgroup::tile_load</a>(gamma_in, gamma_in_payload);</div>
<div class="line"><a id="l00211" name="l00211"></a><span class="lineno">  211</span>            <a class="code hl_function" href="namespacegpu_1_1xetla_1_1subgroup.html#ae110043dd5332ce2a7ef91d2c8ae79ef">subgroup::tile_load</a>(beta_in, beta_in_payload);</div>
<div class="line"><a id="l00212" name="l00212"></a><span class="lineno">  212</span>        }</div>
<div class="line"><a id="l00213" name="l00213"></a><span class="lineno">  213</span>        y_out_payload.init(args-&gt;y_out_ptr, args-&gt;matrix_n, args-&gt;matrix_m,</div>
<div class="line"><a id="l00214" name="l00214"></a><span class="lineno">  214</span>                args-&gt;mat_ld, start_n, start_m);</div>
<div class="line"><a id="l00215" name="l00215"></a><span class="lineno">  215</span>        <span class="keyword">const</span> <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a> sg_rn = 1.0f / sg_tile_n;</div>
<div class="line"><a id="l00216" name="l00216"></a><span class="lineno">  216</span>        <span class="keyword">const</span> <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a> wg_rn = 1.0f / wg_tile_n;</div>
<div class="line"><a id="l00217" name="l00217"></a><span class="lineno">  217</span>        uint32_t slm_store_base_0 = sg_idx * 2 * <span class="keyword">sizeof</span>(<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>)</div>
<div class="line"><a id="l00218" name="l00218"></a><span class="lineno">  218</span>                + sg_idy * wg_size_x * 2 * <span class="keyword">sizeof</span>(<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>) + slm_base;</div>
<div class="line"><a id="l00219" name="l00219"></a><span class="lineno">  219</span>        uint32_t slm_load_base_0</div>
<div class="line"><a id="l00220" name="l00220"></a><span class="lineno">  220</span>                = sg_idy * wg_size_x * 2 * <span class="keyword">sizeof</span>(<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>) + slm_base;</div>
<div class="line"><a id="l00221" name="l00221"></a><span class="lineno">  221</span>        uint32_t slm_store_base_1 = slm_store_base_0</div>
<div class="line"><a id="l00222" name="l00222"></a><span class="lineno">  222</span>                + wg_size_x * wg_size_y * 2 * <span class="keyword">sizeof</span>(<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>);</div>
<div class="line"><a id="l00223" name="l00223"></a><span class="lineno">  223</span>        uint32_t slm_load_base_1 = slm_load_base_0</div>
<div class="line"><a id="l00224" name="l00224"></a><span class="lineno">  224</span>                + wg_size_x * wg_size_y * 2 * <span class="keyword">sizeof</span>(<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>);</div>
<div class="line"><a id="l00225" name="l00225"></a><span class="lineno">  225</span>        uint32_t itr_count = 0;</div>
<div class="line"><a id="l00226" name="l00226"></a><span class="lineno">  226</span> </div>
<div class="line"><a id="l00227" name="l00227"></a><span class="lineno">  227</span>        <span class="keywordflow">for</span> (<span class="keywordtype">int</span> row = start_m; row &lt; args-&gt;matrix_m;</div>
<div class="line"><a id="l00228" name="l00228"></a><span class="lineno">  228</span>                row += wg_num_m * wg_tile_m) {</div>
<div class="line"><a id="l00229" name="l00229"></a><span class="lineno">  229</span>            <span class="keywordflow">if</span> <span class="keyword">constexpr</span> (n_chunks &gt; 1) {</div>
<div class="line"><a id="l00230" name="l00230"></a><span class="lineno">  230</span>                fused_op.init(</div>
<div class="line"><a id="l00231" name="l00231"></a><span class="lineno">  231</span>                        fused_op_args, wg_idx, wg_idy, sg_idx, sg_idy, row);</div>
<div class="line"><a id="l00232" name="l00232"></a><span class="lineno">  232</span>            }</div>
<div class="line"><a id="l00233" name="l00233"></a><span class="lineno">  233</span>            <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;dtype_acc, chunk_size&gt;</a> input;</div>
<div class="line"><a id="l00234" name="l00234"></a><span class="lineno">  234</span>            <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;dtype_acc, 2&gt;</a> mu_m2;</div>
<div class="line"><a id="l00235" name="l00235"></a><span class="lineno">  235</span>            mu_m2[0] = 0;</div>
<div class="line"><a id="l00236" name="l00236"></a><span class="lineno">  236</span>            mu_m2[1] = 0;</div>
<div class="line"><a id="l00237" name="l00237"></a><span class="lineno">  237</span>            <span class="keywordflow">if</span> <span class="keyword">constexpr</span> (n_chunks &gt; 1) {</div>
<div class="line"><a id="l00238" name="l00238"></a><span class="lineno">  238</span>                x_in_payload.init(args-&gt;x_in_ptr, args-&gt;matrix_n,</div>
<div class="line"><a id="l00239" name="l00239"></a><span class="lineno">  239</span>                        args-&gt;matrix_m, args-&gt;mat_ld, start_n, row);</div>
<div class="line"><a id="l00240" name="l00240"></a><span class="lineno">  240</span>            }</div>
<div class="line"><a id="l00241" name="l00241"></a><span class="lineno">  241</span><span class="preprocessor">#pragma unroll</span></div>
<div class="line"><a id="l00242" name="l00242"></a><span class="lineno">  242</span>            <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; n_chunks; i++) {</div>
<div class="line"><a id="l00243" name="l00243"></a><span class="lineno">  243</span>                <a class="code hl_function" href="namespacegpu_1_1xetla_1_1subgroup.html#ae110043dd5332ce2a7ef91d2c8ae79ef">subgroup::tile_load</a>(x_in, x_in_payload);</div>
<div class="line"><a id="l00244" name="l00244"></a><span class="lineno">  244</span>                x_in_payload.update_tdesc(chunk_size);</div>
<div class="line"><a id="l00245" name="l00245"></a><span class="lineno">  245</span>                input = xetla_cvt&lt;dtype_acc, dtype_x&gt;(x_in.<a class="code hl_variable" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html#a7985ceac15e399ebf3000e5693d8801e">reg</a>);</div>
<div class="line"><a id="l00246" name="l00246"></a><span class="lineno">  246</span>                <span class="comment">// &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; fused op pre-processing</span></div>
<div class="line"><a id="l00247" name="l00247"></a><span class="lineno">  247</span>                input = fused_op.pre_op(input);</div>
<div class="line"><a id="l00248" name="l00248"></a><span class="lineno">  248</span>                <span class="comment">// &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; first do sg_level reduction</span></div>
<div class="line"><a id="l00249" name="l00249"></a><span class="lineno">  249</span>                mu_m2[0] += <a class="code hl_function" href="group__xetla__core__math.html#ga015799ae43ea114a01cfce9b7f9f8f7f">xetla_reduce</a>&lt;<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>, <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>, chunk_size,</div>
<div class="line"><a id="l00250" name="l00250"></a><span class="lineno">  250</span>                        <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#a7bbd70f1164d3a7251729e89fffc0609a1d623b89683f9ce4e074de1676d12416">reduce_op::sum</a>&gt;(input);</div>
<div class="line"><a id="l00251" name="l00251"></a><span class="lineno">  251</span>            }</div>
<div class="line"><a id="l00252" name="l00252"></a><span class="lineno">  252</span>            mu_m2[0] *= sg_rn;</div>
<div class="line"><a id="l00253" name="l00253"></a><span class="lineno">  253</span>            <span class="keywordflow">if</span> <span class="keyword">constexpr</span> (n_chunks &gt; 1) {</div>
<div class="line"><a id="l00254" name="l00254"></a><span class="lineno">  254</span>                fused_op.init(</div>
<div class="line"><a id="l00255" name="l00255"></a><span class="lineno">  255</span>                        fused_op_args, wg_idx, wg_idy, sg_idx, sg_idy, row);</div>
<div class="line"><a id="l00256" name="l00256"></a><span class="lineno">  256</span>                x_in_payload.init(args-&gt;x_in_ptr, args-&gt;matrix_n,</div>
<div class="line"><a id="l00257" name="l00257"></a><span class="lineno">  257</span>                        args-&gt;matrix_m, args-&gt;mat_ld, start_n, row);</div>
<div class="line"><a id="l00258" name="l00258"></a><span class="lineno">  258</span>            }</div>
<div class="line"><a id="l00259" name="l00259"></a><span class="lineno">  259</span><span class="preprocessor">#pragma unroll</span></div>
<div class="line"><a id="l00260" name="l00260"></a><span class="lineno">  260</span>            <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; n_chunks; i++) {</div>
<div class="line"><a id="l00261" name="l00261"></a><span class="lineno">  261</span>                <span class="keywordflow">if</span> <span class="keyword">constexpr</span> (n_chunks &gt; 1) {</div>
<div class="line"><a id="l00262" name="l00262"></a><span class="lineno">  262</span>                    <a class="code hl_function" href="namespacegpu_1_1xetla_1_1subgroup.html#ae110043dd5332ce2a7ef91d2c8ae79ef">subgroup::tile_load</a>(x_in, x_in_payload);</div>
<div class="line"><a id="l00263" name="l00263"></a><span class="lineno">  263</span>                    x_in_payload.update_tdesc(chunk_size);</div>
<div class="line"><a id="l00264" name="l00264"></a><span class="lineno">  264</span>                    input = xetla_cvt&lt;dtype_acc, dtype_x&gt;(x_in.<a class="code hl_variable" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html#a7985ceac15e399ebf3000e5693d8801e">reg</a>);</div>
<div class="line"><a id="l00265" name="l00265"></a><span class="lineno">  265</span>                    <span class="comment">// &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; fused op pre-processing</span></div>
<div class="line"><a id="l00266" name="l00266"></a><span class="lineno">  266</span>                    input = fused_op.pre_op(input);</div>
<div class="line"><a id="l00267" name="l00267"></a><span class="lineno">  267</span>                }</div>
<div class="line"><a id="l00268" name="l00268"></a><span class="lineno">  268</span> </div>
<div class="line"><a id="l00269" name="l00269"></a><span class="lineno">  269</span>                <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;dtype_acc, chunk_size&gt;</a> diff</div>
<div class="line"><a id="l00270" name="l00270"></a><span class="lineno">  270</span>                        = input - <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>(mu_m2[0]);</div>
<div class="line"><a id="l00271" name="l00271"></a><span class="lineno">  271</span>                mu_m2[1] += <a class="code hl_function" href="group__xetla__core__math.html#ga015799ae43ea114a01cfce9b7f9f8f7f">xetla_reduce</a>&lt;<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>, <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>, chunk_size,</div>
<div class="line"><a id="l00272" name="l00272"></a><span class="lineno">  272</span>                        <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#a7bbd70f1164d3a7251729e89fffc0609a1d623b89683f9ce4e074de1676d12416">reduce_op::sum</a>&gt;(diff * diff);</div>
<div class="line"><a id="l00273" name="l00273"></a><span class="lineno">  273</span>            }</div>
<div class="line"><a id="l00274" name="l00274"></a><span class="lineno">  274</span>            <span class="comment">// &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; then do wg_level reduction</span></div>
<div class="line"><a id="l00275" name="l00275"></a><span class="lineno">  275</span>            <span class="keywordflow">if</span> <span class="keyword">constexpr</span> (wg_size_x &gt; 1) {</div>
<div class="line"><a id="l00276" name="l00276"></a><span class="lineno">  276</span>                uint32_t slm_store_base = (itr_count &amp; 1) == 0</div>
<div class="line"><a id="l00277" name="l00277"></a><span class="lineno">  277</span>                        ? slm_store_base_0</div>
<div class="line"><a id="l00278" name="l00278"></a><span class="lineno">  278</span>                        : slm_store_base_1;</div>
<div class="line"><a id="l00279" name="l00279"></a><span class="lineno">  279</span>                xetla_store_local&lt;dtype_acc, 2&gt;(slm_store_base, mu_m2);</div>
<div class="line"><a id="l00280" name="l00280"></a><span class="lineno">  280</span>                xetla_fence&lt;memory_kind::shared_local&gt;();</div>
<div class="line"><a id="l00281" name="l00281"></a><span class="lineno">  281</span>                nbarrier.<a class="code hl_function" href="structgpu_1_1xetla_1_1xetla__nbarrier__t.html#af5d567e056aac620a2b7f0728ba5c490">arrive</a>();</div>
<div class="line"><a id="l00282" name="l00282"></a><span class="lineno">  282</span>                uint32_t slm_load_base = (itr_count &amp; 1) == 0 ? slm_load_base_0</div>
<div class="line"><a id="l00283" name="l00283"></a><span class="lineno">  283</span>                                                              : slm_load_base_1;</div>
<div class="line"><a id="l00284" name="l00284"></a><span class="lineno">  284</span>                itr_count += 1;</div>
<div class="line"><a id="l00285" name="l00285"></a><span class="lineno">  285</span>                nbarrier.<a class="code hl_function" href="structgpu_1_1xetla_1_1xetla__nbarrier__t.html#aadcb3c9fab48cd482e554357fdef4ae0">wait</a>();</div>
<div class="line"><a id="l00286" name="l00286"></a><span class="lineno">  286</span> </div>
<div class="line"><a id="l00287" name="l00287"></a><span class="lineno">  287</span>                <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;dtype_acc, wg_size_x * 2&gt;</a> mu_m2_vec</div>
<div class="line"><a id="l00288" name="l00288"></a><span class="lineno">  288</span>                        = xetla_load_local&lt;dtype_acc, wg_size_x * 2&gt;(</div>
<div class="line"><a id="l00289" name="l00289"></a><span class="lineno">  289</span>                                slm_load_base);</div>
<div class="line"><a id="l00290" name="l00290"></a><span class="lineno">  290</span>                <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;dtype_acc, wg_size_x&gt;</a> mu_vec</div>
<div class="line"><a id="l00291" name="l00291"></a><span class="lineno">  291</span>                        = mu_m2_vec.xetla_select&lt;wg_size_x, 2&gt;(0);</div>
<div class="line"><a id="l00292" name="l00292"></a><span class="lineno">  292</span>                <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;dtype_acc, wg_size_x&gt;</a> m2_vec</div>
<div class="line"><a id="l00293" name="l00293"></a><span class="lineno">  293</span>                        = mu_m2_vec.xetla_select&lt;wg_size_x, 2&gt;(1);</div>
<div class="line"><a id="l00294" name="l00294"></a><span class="lineno">  294</span>                mu_m2 = parallel_mu_m2_t&lt;dtype_acc, wg_size_x, sg_tile_n&gt;::call(</div>
<div class="line"><a id="l00295" name="l00295"></a><span class="lineno">  295</span>                        mu_vec, m2_vec);</div>
<div class="line"><a id="l00296" name="l00296"></a><span class="lineno">  296</span>            }</div>
<div class="line"><a id="l00297" name="l00297"></a><span class="lineno">  297</span>            <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a> mu = mu_m2[0];</div>
<div class="line"><a id="l00298" name="l00298"></a><span class="lineno">  298</span>            <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a> m2 = mu_m2[1];</div>
<div class="line"><a id="l00299" name="l00299"></a><span class="lineno">  299</span>            <a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a> rs = <a class="code hl_function" href="group__xetla__core__math.html#gaabd5a2631196ee6ae503c6b051ef8290">xetla_rsqrt</a>(m2 * wg_rn + args-&gt;epsilon);</div>
<div class="line"><a id="l00300" name="l00300"></a><span class="lineno">  300</span> </div>
<div class="line"><a id="l00301" name="l00301"></a><span class="lineno">  301</span>            <span class="keywordflow">if</span> <span class="keyword">constexpr</span> (store_for_bwd) {</div>
<div class="line"><a id="l00302" name="l00302"></a><span class="lineno">  302</span>                <span class="keywordflow">if</span> (sg_idx == 0) {</div>
<div class="line"><a id="l00303" name="l00303"></a><span class="lineno">  303</span>                    <a class="code hl_function" href="group__xetla__core__memory.html#ga03572dc0e489ddc1e352f82914d8d4c9">xetla_store_global</a>&lt;<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>, 1, <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#a2cf716afc0eaf0c0a87c572193d7ca76a6bad67b5e8990b5f40b54dddd984ea08">data_size::default_size</a>,</div>
<div class="line"><a id="l00304" name="l00304"></a><span class="lineno">  304</span>                            <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#a23ee6c8b836eb63360633951d75d7e30aa9c64c25d98516dabbeaa44fa4b798fe">cache_hint::write_back</a>, <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#a23ee6c8b836eb63360633951d75d7e30aa9c64c25d98516dabbeaa44fa4b798fe">cache_hint::write_back</a>&gt;(</div>
<div class="line"><a id="l00305" name="l00305"></a><span class="lineno">  305</span>                            args-&gt;mu_ptr, row * <span class="keyword">sizeof</span>(<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>),</div>
<div class="line"><a id="l00306" name="l00306"></a><span class="lineno">  306</span>                            <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;dtype_acc, 1&gt;</a>(mu));</div>
<div class="line"><a id="l00307" name="l00307"></a><span class="lineno">  307</span>                    <a class="code hl_function" href="group__xetla__core__memory.html#ga03572dc0e489ddc1e352f82914d8d4c9">xetla_store_global</a>&lt;<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>, 1, <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#a2cf716afc0eaf0c0a87c572193d7ca76a6bad67b5e8990b5f40b54dddd984ea08">data_size::default_size</a>,</div>
<div class="line"><a id="l00308" name="l00308"></a><span class="lineno">  308</span>                            <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#a23ee6c8b836eb63360633951d75d7e30aa9c64c25d98516dabbeaa44fa4b798fe">cache_hint::write_back</a>, <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#a23ee6c8b836eb63360633951d75d7e30aa9c64c25d98516dabbeaa44fa4b798fe">cache_hint::write_back</a>&gt;(</div>
<div class="line"><a id="l00309" name="l00309"></a><span class="lineno">  309</span>                            args-&gt;rs_ptr, row * <span class="keyword">sizeof</span>(<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>),</div>
<div class="line"><a id="l00310" name="l00310"></a><span class="lineno">  310</span>                            <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;dtype_acc, 1&gt;</a>(rs));</div>
<div class="line"><a id="l00311" name="l00311"></a><span class="lineno">  311</span>                }</div>
<div class="line"><a id="l00312" name="l00312"></a><span class="lineno">  312</span>            }</div>
<div class="line"><a id="l00313" name="l00313"></a><span class="lineno">  313</span>            <span class="comment">// to generate mixed instruction</span></div>
<div class="line"><a id="l00314" name="l00314"></a><span class="lineno">  314</span>            <span class="keyword">constexpr</span> uint32_t <a class="code hl_define" href="gemm__softmax_8cpp.html#ad1ab836291a12a078c0b378699e1de73">SIMD</a> = 64 / <span class="keyword">sizeof</span>(<a class="code hl_typedef" href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">dtype_acc</a>);</div>
<div class="line"><a id="l00315" name="l00315"></a><span class="lineno">  315</span>            <span class="keywordflow">if</span> <span class="keyword">constexpr</span> (chunk_size &gt; 1) {</div>
<div class="line"><a id="l00316" name="l00316"></a><span class="lineno">  316</span>                gamma_in_payload.init(args-&gt;gamma_ptr, args-&gt;matrix_n, 1,</div>
<div class="line"><a id="l00317" name="l00317"></a><span class="lineno">  317</span>                        args-&gt;mat_ld, start_n, 0);</div>
<div class="line"><a id="l00318" name="l00318"></a><span class="lineno">  318</span>                beta_in_payload.init(args-&gt;beta_ptr, args-&gt;matrix_n, 1,</div>
<div class="line"><a id="l00319" name="l00319"></a><span class="lineno">  319</span>                        args-&gt;mat_ld, start_n, 0);</div>
<div class="line"><a id="l00320" name="l00320"></a><span class="lineno">  320</span>            }</div>
<div class="line"><a id="l00321" name="l00321"></a><span class="lineno">  321</span> </div>
<div class="line"><a id="l00322" name="l00322"></a><span class="lineno">  322</span>            <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;dtype_acc, chunk_size&gt;</a> output;</div>
<div class="line"><a id="l00323" name="l00323"></a><span class="lineno">  323</span> </div>
<div class="line"><a id="l00324" name="l00324"></a><span class="lineno">  324</span>            <span class="keywordflow">if</span> <span class="keyword">constexpr</span> (n_chunks &gt; 1) {</div>
<div class="line"><a id="l00325" name="l00325"></a><span class="lineno">  325</span>                fused_op.init(</div>
<div class="line"><a id="l00326" name="l00326"></a><span class="lineno">  326</span>                        fused_op_args, wg_idx, wg_idy, sg_idx, sg_idy, row);</div>
<div class="line"><a id="l00327" name="l00327"></a><span class="lineno">  327</span>                x_in_payload.init(args-&gt;x_in_ptr, args-&gt;matrix_n,</div>
<div class="line"><a id="l00328" name="l00328"></a><span class="lineno">  328</span>                        args-&gt;matrix_m, args-&gt;mat_ld, start_n, row);</div>
<div class="line"><a id="l00329" name="l00329"></a><span class="lineno">  329</span>            }</div>
<div class="line"><a id="l00330" name="l00330"></a><span class="lineno">  330</span>            <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;dtype_acc, chunk_size&gt;</a> beta;</div>
<div class="line"><a id="l00331" name="l00331"></a><span class="lineno">  331</span>            <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;dtype_acc, chunk_size&gt;</a> gamma;</div>
<div class="line"><a id="l00332" name="l00332"></a><span class="lineno">  332</span><span class="preprocessor">#pragma unroll</span></div>
<div class="line"><a id="l00333" name="l00333"></a><span class="lineno">  333</span>            <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; n_chunks; i++) {</div>
<div class="line"><a id="l00334" name="l00334"></a><span class="lineno">  334</span>                <span class="keywordflow">if</span> <span class="keyword">constexpr</span> (n_chunks &gt; 1) {</div>
<div class="line"><a id="l00335" name="l00335"></a><span class="lineno">  335</span>                    <a class="code hl_function" href="namespacegpu_1_1xetla_1_1subgroup.html#ae110043dd5332ce2a7ef91d2c8ae79ef">subgroup::tile_load</a>(gamma_in, gamma_in_payload);</div>
<div class="line"><a id="l00336" name="l00336"></a><span class="lineno">  336</span>                    gamma_in_payload.update_tdesc(chunk_size);</div>
<div class="line"><a id="l00337" name="l00337"></a><span class="lineno">  337</span> </div>
<div class="line"><a id="l00338" name="l00338"></a><span class="lineno">  338</span>                    <a class="code hl_function" href="namespacegpu_1_1xetla_1_1subgroup.html#ae110043dd5332ce2a7ef91d2c8ae79ef">subgroup::tile_load</a>(beta_in, beta_in_payload);</div>
<div class="line"><a id="l00339" name="l00339"></a><span class="lineno">  339</span>                    beta_in_payload.update_tdesc(chunk_size);</div>
<div class="line"><a id="l00340" name="l00340"></a><span class="lineno">  340</span> </div>
<div class="line"><a id="l00341" name="l00341"></a><span class="lineno">  341</span>                    <a class="code hl_function" href="namespacegpu_1_1xetla_1_1subgroup.html#ae110043dd5332ce2a7ef91d2c8ae79ef">subgroup::tile_load</a>(x_in, x_in_payload);</div>
<div class="line"><a id="l00342" name="l00342"></a><span class="lineno">  342</span>                    x_in_payload.update_tdesc(chunk_size);</div>
<div class="line"><a id="l00343" name="l00343"></a><span class="lineno">  343</span>                    input = xetla_cvt&lt;dtype_acc, dtype_x&gt;(x_in.<a class="code hl_variable" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html#a7985ceac15e399ebf3000e5693d8801e">reg</a>);</div>
<div class="line"><a id="l00344" name="l00344"></a><span class="lineno">  344</span>                    <span class="comment">// &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; fused op pre-processing</span></div>
<div class="line"><a id="l00345" name="l00345"></a><span class="lineno">  345</span>                    input = fused_op.pre_op(input);</div>
<div class="line"><a id="l00346" name="l00346"></a><span class="lineno">  346</span>                }</div>
<div class="line"><a id="l00347" name="l00347"></a><span class="lineno">  347</span>                <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;dtype_acc, chunk_size&gt;</a> beta</div>
<div class="line"><a id="l00348" name="l00348"></a><span class="lineno">  348</span>                        = xetla_cvt&lt;dtype_acc, dtype_weight, chunk_size&gt;(</div>
<div class="line"><a id="l00349" name="l00349"></a><span class="lineno">  349</span>                                beta_in.<a class="code hl_variable" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html#a7985ceac15e399ebf3000e5693d8801e">reg</a>);</div>
<div class="line"><a id="l00350" name="l00350"></a><span class="lineno">  350</span>                <a class="code hl_typedef" href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">xetla_vector&lt;dtype_acc, chunk_size&gt;</a> gamma</div>
<div class="line"><a id="l00351" name="l00351"></a><span class="lineno">  351</span>                        = xetla_cvt&lt;dtype_acc, dtype_weight&gt;(gamma_in.<a class="code hl_variable" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html#a7985ceac15e399ebf3000e5693d8801e">reg</a>);</div>
<div class="line"><a id="l00352" name="l00352"></a><span class="lineno">  352</span> </div>
<div class="line"><a id="l00353" name="l00353"></a><span class="lineno">  353</span>                output = beta + (rs * (input - mu)) * gamma;</div>
<div class="line"><a id="l00354" name="l00354"></a><span class="lineno">  354</span>                <span class="comment">// &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; fused op post-processing</span></div>
<div class="line"><a id="l00355" name="l00355"></a><span class="lineno">  355</span>                output = fused_op.post_op(output);</div>
<div class="line"><a id="l00356" name="l00356"></a><span class="lineno">  356</span>                y_out.<a class="code hl_variable" href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html#a7985ceac15e399ebf3000e5693d8801e">reg</a> = xetla_cvt&lt;dtype_y, dtype_acc, chunk_size&gt;(output);</div>
<div class="line"><a id="l00357" name="l00357"></a><span class="lineno">  357</span>                <a class="code hl_function" href="namespacegpu_1_1xetla_1_1subgroup.html#aca86d85d16cd70ce167a4819af5d29ef">subgroup::tile_store</a>&lt;<a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#a23ee6c8b836eb63360633951d75d7e30aaf9f686f9eb416162d298446a94cfafb">cache_hint::uncached</a>,</div>
<div class="line"><a id="l00358" name="l00358"></a><span class="lineno">  358</span>                        <a class="code hl_enumvalue" href="namespacegpu_1_1xetla.html#a23ee6c8b836eb63360633951d75d7e30aa9c64c25d98516dabbeaa44fa4b798fe">cache_hint::write_back</a>&gt;(y_out, y_out_payload);</div>
<div class="line"><a id="l00359" name="l00359"></a><span class="lineno">  359</span>                y_out_payload.update_tdesc(chunk_size);</div>
<div class="line"><a id="l00360" name="l00360"></a><span class="lineno">  360</span>            }</div>
<div class="line"><a id="l00361" name="l00361"></a><span class="lineno">  361</span>            x_in_payload.update_tdesc(</div>
<div class="line"><a id="l00362" name="l00362"></a><span class="lineno">  362</span>                    wg_num_m * wg_tile_m * args-&gt;mat_ld - sg_tile_n);</div>
<div class="line"><a id="l00363" name="l00363"></a><span class="lineno">  363</span>            y_out_payload.update_tdesc(</div>
<div class="line"><a id="l00364" name="l00364"></a><span class="lineno">  364</span>                    wg_num_m * wg_tile_m * args-&gt;mat_ld - sg_tile_n);</div>
<div class="line"><a id="l00365" name="l00365"></a><span class="lineno">  365</span>        }</div>
<div class="line"><a id="l00366" name="l00366"></a><span class="lineno">  366</span>    }</div>
<div class="line"><a id="l00367" name="l00367"></a><span class="lineno">  367</span>};</div>
<div class="line"><a id="l00368" name="l00368"></a><span class="lineno">  368</span> </div>
<div class="line"><a id="l00369" name="l00369"></a><span class="lineno">  369</span>} <span class="comment">// namespace gpu::xetla::kernel</span></div>
<div class="ttc" id="aclassgpu_1_1xetla_1_1xetla__exec__item_html"><div class="ttname"><a href="classgpu_1_1xetla_1_1xetla__exec__item.html">gpu::xetla::xetla_exec_item</a></div><div class="ttdoc">The item struct to explict identify the group / local id information.</div><div class="ttdef"><b>Definition:</b> execution_item.hpp:31</div></div>
<div class="ttc" id="aclassgpu_1_1xetla_1_1xetla__exec__item_html_a740f63e18f312690bfae48783da27644"><div class="ttname"><a href="classgpu_1_1xetla_1_1xetla__exec__item.html#a740f63e18f312690bfae48783da27644">gpu::xetla::xetla_exec_item::get_local_linear_id</a></div><div class="ttdeci">uint32_t get_local_linear_id() const</div><div class="ttdoc">Returns local linear id in the work group.</div><div class="ttdef"><b>Definition:</b> execution_item.hpp:66</div></div>
<div class="ttc" id="aclassgpu_1_1xetla_1_1xetla__exec__item_html_ac46733b57cc482439c9f7e13916c1514"><div class="ttname"><a href="classgpu_1_1xetla_1_1xetla__exec__item.html#ac46733b57cc482439c9f7e13916c1514">gpu::xetla::xetla_exec_item::get_group</a></div><div class="ttdeci">uint32_t get_group(int dimension) const</div><div class="ttdoc">Returns the group id for the requested dimensions.</div><div class="ttdef"><b>Definition:</b> execution_item.hpp:40</div></div>
<div class="ttc" id="acommon_2core_2common_8hpp_html_a9ed53999886ec13b86a4fe2e0fc16765"><div class="ttname"><a href="common_2core_2common_8hpp.html#a9ed53999886ec13b86a4fe2e0fc16765">__XETLA_API</a></div><div class="ttdeci">#define __XETLA_API</div><div class="ttdef"><b>Definition:</b> common.hpp:43</div></div>
<div class="ttc" id="aexperimental_2kernel_2layer__norm_2api_8hpp_html"><div class="ttname"><a href="experimental_2kernel_2layer__norm_2api_8hpp.html">api.hpp</a></div><div class="ttdoc">C++ API.</div></div>
<div class="ttc" id="aexperimental_2kernel_2layer__norm_2common_8hpp_html"><div class="ttname"><a href="experimental_2kernel_2layer__norm_2common_8hpp.html">common.hpp</a></div><div class="ttdoc">C++ API.</div></div>
<div class="ttc" id="agemm__softmax_8cpp_html_ad1ab836291a12a078c0b378699e1de73"><div class="ttname"><a href="gemm__softmax_8cpp.html#ad1ab836291a12a078c0b378699e1de73">SIMD</a></div><div class="ttdeci">#define SIMD</div><div class="ttdef"><b>Definition:</b> gemm_softmax.cpp:23</div></div>
<div class="ttc" id="agroup__xetla__core__arch__config_html_gaa5a2713edb27d6fed88a3c61673556f1"><div class="ttname"><a href="group__xetla__core__arch__config.html#gaa5a2713edb27d6fed88a3c61673556f1">gpu::xetla::gpu_arch</a></div><div class="ttdeci">gpu_arch</div><div class="ttdef"><b>Definition:</b> arch_config.hpp:28</div></div>
<div class="ttc" id="agroup__xetla__core__arch__config_html_ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130"><div class="ttname"><a href="group__xetla__core__arch__config.html#ggaa5a2713edb27d6fed88a3c61673556f1a8fde9df1bf2567b76160d1beedca3130">gpu::xetla::gpu_arch::Xe</a></div><div class="ttdeci">@ Xe</div></div>
<div class="ttc" id="agroup__xetla__core__base__types_html_ga8cf5d016d24c8870706e20c376287e04"><div class="ttname"><a href="group__xetla__core__base__types.html#ga8cf5d016d24c8870706e20c376287e04">gpu::xetla::xetla_vector</a></div><div class="ttdeci">__ESIMD_NS::simd&lt; native_type_t&lt; Ty &gt;, N &gt; xetla_vector</div><div class="ttdoc">wrapper for xetla_vector.</div><div class="ttdef"><b>Definition:</b> base_types.hpp:167</div></div>
<div class="ttc" id="agroup__xetla__core__math_html_ga015799ae43ea114a01cfce9b7f9f8f7f"><div class="ttname"><a href="group__xetla__core__math.html#ga015799ae43ea114a01cfce9b7f9f8f7f">gpu::xetla::xetla_reduce</a></div><div class="ttdeci">__XETLA_API T0 xetla_reduce(xetla_vector&lt; T1, SZ &gt; v)</div><div class="ttdoc">Performs reduction over elements of the input vector.</div><div class="ttdef"><b>Definition:</b> math_general.hpp:520</div></div>
<div class="ttc" id="agroup__xetla__core__math_html_gaabd5a2631196ee6ae503c6b051ef8290"><div class="ttname"><a href="group__xetla__core__math.html#gaabd5a2631196ee6ae503c6b051ef8290">gpu::xetla::xetla_rsqrt</a></div><div class="ttdeci">__XETLA_API xetla_vector&lt; T, SZ &gt; xetla_rsqrt(xetla_vector&lt; T, SZ &gt; src, Sat sat={})</div><div class="ttdoc">Calculate the inversion of square root, i.e.</div><div class="ttdef"><b>Definition:</b> math_general.hpp:374</div></div>
<div class="ttc" id="agroup__xetla__core__memory_html_ga03572dc0e489ddc1e352f82914d8d4c9"><div class="ttname"><a href="group__xetla__core__memory.html#ga03572dc0e489ddc1e352f82914d8d4c9">gpu::xetla::xetla_store_global</a></div><div class="ttdeci">__XETLA_API void xetla_store_global(Ty *p, xetla_vector&lt; uint32_t, N &gt; offsets, xetla_vector&lt; Ty, N *NElts &gt; vals, xetla_mask&lt; N &gt; pred=1)</div><div class="ttdoc">Stateless scattered store.</div><div class="ttdef"><b>Definition:</b> memory.hpp:302</div></div>
<div class="ttc" id="agroup__xetla__util__named__barrier_html_ggaad562941909edcd788dd091e31841fe1a4c44b0011c9c00f486a2d65f1293de8f"><div class="ttname"><a href="group__xetla__util__named__barrier.html#ggaad562941909edcd788dd091e31841fe1a4c44b0011c9c00f486a2d65f1293de8f">gpu::xetla::nbarrier_role::producer_consumer</a></div><div class="ttdeci">@ producer_consumer</div></div>
<div class="ttc" id="alayer__norm_2config_8hpp_html"><div class="ttname"><a href="layer__norm_2config_8hpp.html">config.hpp</a></div><div class="ttdoc">C++ API.</div></div>
<div class="ttc" id="alayer__norm__fused__op__fwd__xe_8hpp_html"><div class="ttname"><a href="layer__norm__fused__op__fwd__xe_8hpp.html">layer_norm_fused_op_fwd_xe.hpp</a></div><div class="ttdoc">C++ API.</div></div>
<div class="ttc" id="anamespacegpu_1_1xetla_1_1kernel_html"><div class="ttname"><a href="namespacegpu_1_1xetla_1_1kernel.html">gpu::xetla::kernel</a></div><div class="ttdef"><b>Definition:</b> api.hpp:24</div></div>
<div class="ttc" id="anamespacegpu_1_1xetla_1_1subgroup_html_aca86d85d16cd70ce167a4819af5d29ef"><div class="ttname"><a href="namespacegpu_1_1xetla_1_1subgroup.html#aca86d85d16cd70ce167a4819af5d29ef">gpu::xetla::subgroup::tile_store</a></div><div class="ttdeci">__XETLA_API std::enable_if_t&lt; detail::check_store_type&lt; tile_t, payload_t &gt;::is_global_2d_xe &gt; tile_store(tile_t &amp;tile, payload_t &amp;payload)</div><div class="ttdoc">Is the func storing data from register file to global memory.</div><div class="ttdef"><b>Definition:</b> store_xe.hpp:156</div></div>
<div class="ttc" id="anamespacegpu_1_1xetla_1_1subgroup_html_ae110043dd5332ce2a7ef91d2c8ae79ef"><div class="ttname"><a href="namespacegpu_1_1xetla_1_1subgroup.html#ae110043dd5332ce2a7ef91d2c8ae79ef">gpu::xetla::subgroup::tile_load</a></div><div class="ttdeci">__XETLA_API std::enable_if_t&lt; detail::check_load_type&lt; tile_t, payload_t &gt;::is_global_2d_xe &gt; tile_load(tile_t &amp;tile, payload_t &amp;payload)</div><div class="ttdoc">This function loads data from 2D memory surface.</div><div class="ttdef"><b>Definition:</b> load_xe.hpp:138</div></div>
<div class="ttc" id="anamespacegpu_1_1xetla_html_a23ee6c8b836eb63360633951d75d7e30aa9c64c25d98516dabbeaa44fa4b798fe"><div class="ttname"><a href="namespacegpu_1_1xetla.html#a23ee6c8b836eb63360633951d75d7e30aa9c64c25d98516dabbeaa44fa4b798fe">gpu::xetla::cache_hint::write_back</a></div><div class="ttdeci">@ write_back</div></div>
<div class="ttc" id="anamespacegpu_1_1xetla_html_a23ee6c8b836eb63360633951d75d7e30aaf9f686f9eb416162d298446a94cfafb"><div class="ttname"><a href="namespacegpu_1_1xetla.html#a23ee6c8b836eb63360633951d75d7e30aaf9f686f9eb416162d298446a94cfafb">gpu::xetla::cache_hint::uncached</a></div><div class="ttdeci">@ uncached</div></div>
<div class="ttc" id="anamespacegpu_1_1xetla_html_a2cf716afc0eaf0c0a87c572193d7ca76a6bad67b5e8990b5f40b54dddd984ea08"><div class="ttname"><a href="namespacegpu_1_1xetla.html#a2cf716afc0eaf0c0a87c572193d7ca76a6bad67b5e8990b5f40b54dddd984ea08">gpu::xetla::data_size::default_size</a></div><div class="ttdeci">@ default_size</div></div>
<div class="ttc" id="anamespacegpu_1_1xetla_html_a51137fd81d0d9d2156525a1e279432aaa78506882d645395a052df8b01a927395"><div class="ttname"><a href="namespacegpu_1_1xetla.html#a51137fd81d0d9d2156525a1e279432aaa78506882d645395a052df8b01a927395">gpu::xetla::reg_layout::tiled</a></div><div class="ttdeci">@ tiled</div></div>
<div class="ttc" id="anamespacegpu_1_1xetla_html_a7bbd70f1164d3a7251729e89fffc0609a1d623b89683f9ce4e074de1676d12416"><div class="ttname"><a href="namespacegpu_1_1xetla.html#a7bbd70f1164d3a7251729e89fffc0609a1d623b89683f9ce4e074de1676d12416">gpu::xetla::reduce_op::sum</a></div><div class="ttdeci">@ sum</div></div>
<div class="ttc" id="anamespacegpu_1_1xetla_html_a7f225ed816e841c1d31414d872dae59da9c70933aff6b2a6d08c687a6cbb6b765"><div class="ttname"><a href="namespacegpu_1_1xetla.html#a7f225ed816e841c1d31414d872dae59da9c70933aff6b2a6d08c687a6cbb6b765">gpu::xetla::mem_space::global</a></div><div class="ttdeci">@ global</div></div>
<div class="ttc" id="anamespacegpu_1_1xetla_html_aa8afe1d12e7777419fb6ea09534a0aa7a5baf9a1aba8e4b6fa44c20aafacc4233"><div class="ttname"><a href="namespacegpu_1_1xetla.html#aa8afe1d12e7777419fb6ea09534a0aa7a5baf9a1aba8e4b6fa44c20aafacc4233">gpu::xetla::msg_type::block_1d</a></div><div class="ttdeci">@ block_1d</div></div>
<div class="ttc" id="anamespacegpu_1_1xetla_html_af4a355a1806510c5515fad16f5910561a641fabb8e5e7d1d0333e2c9c384f959c"><div class="ttname"><a href="namespacegpu_1_1xetla.html#af4a355a1806510c5515fad16f5910561a641fabb8e5e7d1d0333e2c9c384f959c">gpu::xetla::mem_layout::row_major</a></div><div class="ttdeci">@ row_major</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w1874b19073e1f423699cbfc0c66a9c60_html_af7efa4be4adc4580d377e4ca7b15e95a"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w1874b19073e1f423699cbfc0c66a9c60.html#af7efa4be4adc4580d377e4ca7b15e95a">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::parallel_mu_m2_t::call</a></div><div class="ttdeci">static xetla_vector&lt; T, 2 &gt; call(xetla_vector&lt; T, SZ &gt; mu_vec, xetla_vector&lt; T, SZ &gt; m2_vec)</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:130</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w34b86ab2795b16851d713a3deae4a27a_html_a03880fd66a722a0d96638ac4bca9604e"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w34b86ab2795b16851d713a3deae4a27a.html#a03880fd66a722a0d96638ac4bca9604e">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::parallel_mu_m2_t&lt; T, 1, N &gt;::call</a></div><div class="ttdeci">static xetla_vector&lt; T, 2 &gt; call(xetla_vector&lt; T, 1 &gt; mu_vec, xetla_vector&lt; T, 1 &gt; m2_vec)</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:157</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0_html_a1390eb14f00da2b114da33efbfe2f2e1"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#a1390eb14f00da2b114da33efbfe2f2e1">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::arguments_t::matrix_n</a></div><div class="ttdeci">uint32_t matrix_n</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:118</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0_html_a2ea50368a4261455d64b75376c19295d"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#a2ea50368a4261455d64b75376c19295d">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::arguments_t::matrix_m</a></div><div class="ttdeci">uint32_t matrix_m</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:117</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0_html_a576f7319fa331610b05b580ca0432841"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#a576f7319fa331610b05b580ca0432841">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::arguments_t::mat_ld</a></div><div class="ttdeci">uint32_t mat_ld</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:119</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0_html_aa808e348768e4008a264c9af99e995ca"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#aa808e348768e4008a264c9af99e995ca">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::arguments_t::y_out_ptr</a></div><div class="ttdeci">dtype_y * y_out_ptr</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:114</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0_html_ab20a6582052f44dc57210137461b9b87"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#ab20a6582052f44dc57210137461b9b87">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::arguments_t::rs_ptr</a></div><div class="ttdeci">dtype_acc * rs_ptr</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:115</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0_html_abaa8295ca8928c30930ff501215f1142"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#abaa8295ca8928c30930ff501215f1142">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::arguments_t::x_in_ptr</a></div><div class="ttdeci">dtype_x * x_in_ptr</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:111</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0_html_abd672da166c6339a297a24338d8bfcbc"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#abd672da166c6339a297a24338d8bfcbc">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::arguments_t::beta_ptr</a></div><div class="ttdeci">dtype_weight * beta_ptr</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:113</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0_html_aeedc6bd926f640e1ca431d76559cb11b"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#aeedc6bd926f640e1ca431d76559cb11b">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::arguments_t::gamma_ptr</a></div><div class="ttdeci">dtype_weight * gamma_ptr</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:112</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0_html_af7e516fb6543029649d2b79f1e59c052"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__w678e01627b39f80350d654b1d9c8f6b0.html#af7e516fb6543029649d2b79f1e59c052">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::arguments_t::mu_ptr</a></div><div class="ttdeci">dtype_acc * mu_ptr</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:116</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766_html_a16904944686fbfe4b4ece2ea5753491d"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a16904944686fbfe4b4ece2ea5753491d">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::layer_norm_attr</a></div><div class="ttdeci">layer_norm_attr_ layer_norm_attr</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:47</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766_html_a1f8407b53863be4dd2e20aaf4f534508"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a1f8407b53863be4dd2e20aaf4f534508">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::call</a></div><div class="ttdeci">static __XETLA_API void call(xetla_exec_item&lt; 3 &gt; &amp;ei, arguments_t *args, uint32_t slm_base=0, uint32_t nbarrier_base=0, ln_fused_op_arguments_t *fused_op_args=nullptr)</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:174</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766_html_a28616c528c93bcbf9dfcf4b5e232b035"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a28616c528c93bcbf9dfcf4b5e232b035">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::dtype_acc</a></div><div class="ttdeci">dtype_acc_ dtype_acc</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:46</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766_html_a301cef1451b18ca22bf1ee2fddc59889"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a301cef1451b18ca22bf1ee2fddc59889">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::dtype_x</a></div><div class="ttdeci">dtype_x_ dtype_x</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:43</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766_html_a5597dc4fa9d1e4bcd545741ce1d252b9"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a5597dc4fa9d1e4bcd545741ce1d252b9">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::work_group_t</a></div><div class="ttdeci">work_group_t&lt; wg_size_x *wg_size_y &gt; work_group_t</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:67</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766_html_a77e9ac25570625f11b5de2de9e63e781"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a77e9ac25570625f11b5de2de9e63e781">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::dtype_y</a></div><div class="ttdeci">dtype_y_ dtype_y</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:44</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766_html_a9ec85363866af435edc21470ca16802d"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#a9ec85363866af435edc21470ca16802d">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::ln_fwd_fused_op</a></div><div class="ttdeci">ln_fwd_fused_op_ ln_fwd_fused_op</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:48</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766_html_aed6a9be25e7ad6eac7563d4a70166709"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#aed6a9be25e7ad6eac7563d4a70166709">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::ln_fused_op_arguments_t</a></div><div class="ttdeci">typename ln_fwd_fused_op::arguments_t ln_fused_op_arguments_t</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:49</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766_html_afabd0691dfb4592ec312cff8b8b46767"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_3_01dtype__x___00_01dtype__y___00_01dtype__wf6b635a4d65490f92949ed8f8e6c9766.html#afabd0691dfb4592ec312cff8b8b46767">gpu::xetla::kernel::layer_norm_fwd_t&lt; dtype_x_, dtype_y_, dtype_weight_, dtype_acc_, layer_norm_attr_, store_for_bwd_, gpu_arch::Xe, ln_fwd_fused_op_ &gt;::dtype_weight</a></div><div class="ttdeci">dtype_weight_ dtype_weight</div><div class="ttdef"><b>Definition:</b> layer_norm_fwd_xe.hpp:45</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t_html"><div class="ttname"><a href="structgpu_1_1xetla_1_1kernel_1_1layer__norm__fwd__t.html">gpu::xetla::kernel::layer_norm_fwd_t</a></div><div class="ttdef"><b>Definition:</b> api.hpp:43</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1subgroup_1_1mem__payload__t_html"><div class="ttname"><a href="structgpu_1_1xetla_1_1subgroup_1_1mem__payload__t.html">gpu::xetla::subgroup::mem_payload_t</a></div><div class="ttdoc">Is to illustrate the memory information.</div><div class="ttdef"><b>Definition:</b> api.hpp:46</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1subgroup_1_1tile__desc__t_html"><div class="ttname"><a href="structgpu_1_1xetla_1_1subgroup_1_1tile__desc__t.html">gpu::xetla::subgroup::tile_desc_t</a></div><div class="ttdoc">Is to illustrate the tile information about a sub matrix.</div><div class="ttdef"><b>Definition:</b> api.hpp:69</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1subgroup_1_1tile__t_html"><div class="ttname"><a href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html">gpu::xetla::subgroup::tile_t</a></div><div class="ttdoc">Is a struct contains some register file.</div><div class="ttdef"><b>Definition:</b> api.hpp:104</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1subgroup_1_1tile__t_html_a7985ceac15e399ebf3000e5693d8801e"><div class="ttname"><a href="structgpu_1_1xetla_1_1subgroup_1_1tile__t.html#a7985ceac15e399ebf3000e5693d8801e">gpu::xetla::subgroup::tile_t::reg</a></div><div class="ttdeci">xetla_vector&lt; dtype, tile_desc::tile_elems &gt; reg</div><div class="ttdef"><b>Definition:</b> api.hpp:107</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1xetla__nbarrier__t_html"><div class="ttname"><a href="structgpu_1_1xetla_1_1xetla__nbarrier__t.html">gpu::xetla::xetla_nbarrier_t</a></div><div class="ttdoc">xetla nbarrier definition API.</div><div class="ttdef"><b>Definition:</b> raw_send_nbarrier.hpp:42</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1xetla__nbarrier__t_html_aadcb3c9fab48cd482e554357fdef4ae0"><div class="ttname"><a href="structgpu_1_1xetla_1_1xetla__nbarrier__t.html#aadcb3c9fab48cd482e554357fdef4ae0">gpu::xetla::xetla_nbarrier_t::wait</a></div><div class="ttdeci">__XETLA_API void wait()</div><div class="ttdoc">named barrier wait within subgroup.</div><div class="ttdef"><b>Definition:</b> raw_send_nbarrier.hpp:75</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1xetla__nbarrier__t_html_ae77b04d911300020ab3dff5f9d4f9d54"><div class="ttname"><a href="structgpu_1_1xetla_1_1xetla__nbarrier__t.html#ae77b04d911300020ab3dff5f9d4f9d54">gpu::xetla::xetla_nbarrier_t::init_nbarrier</a></div><div class="ttdeci">__XETLA_API void init_nbarrier(uint8_t nbarrier_id, nbarrier_role role=nbarrier_role::producer_consumer)</div><div class="ttdef"><b>Definition:</b> raw_send_nbarrier.hpp:54</div></div>
<div class="ttc" id="astructgpu_1_1xetla_1_1xetla__nbarrier__t_html_af5d567e056aac620a2b7f0728ba5c490"><div class="ttname"><a href="structgpu_1_1xetla_1_1xetla__nbarrier__t.html#af5d567e056aac620a2b7f0728ba5c490">gpu::xetla::xetla_nbarrier_t::arrive</a></div><div class="ttdeci">__XETLA_API void arrive()</div><div class="ttdoc">named barrier signal from subgroup.</div><div class="ttdef"><b>Definition:</b> raw_send_nbarrier.hpp:64</div></div>
</div><!-- fragment --></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="dir_d44c64559bbebec7f509842c48db8b23.html">include</a></li><li class="navelem"><a class="el" href="dir_8966848d4591654ab1db845bb311f08b.html">experimental</a></li><li class="navelem"><a class="el" href="dir_da6d88b16527b966b2bed57376e43e91.html">kernel</a></li><li class="navelem"><a class="el" href="dir_142ba5024da2864de75251985ba3a4cc.html">layer_norm</a></li><li class="navelem"><a class="el" href="layer__norm__fwd__xe_8hpp.html">layer_norm_fwd_xe.hpp</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5 </li>
  </ul>
</div>
</body>
</html>
